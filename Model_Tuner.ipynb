{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3de1277",
   "metadata": {
    "id": "c3de1277"
   },
   "source": [
    "# Predictive Maintenance: Remaining Useful Life (RUL) Prediction - Hyperparameter Tuning\n",
    "\n",
    "This notebook focuses on optimizing the Long Short-Term Memory (LSTM) neural network model for Remaining Useful Life (RUL) prediction using **Keras Tuner**. Building on the data preprocessing and initial model definition, this phase aims to systematically explore different model architectures and hyperparameters to achieve the best possible performance and generalization.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Setup and Data Preparation](#1.-Setup-and-Data-Preparation)\n",
    "    * [1.1 Importing Libraries and Loading Preprocessed Data](#1.1-Importing-Libraries-and-Loading-Preprocessed-Data)\n",
    "    * [1.2 Creating Training and Validation Sets](#1.2-Creating-Training-and-Validation-Sets)\n",
    "2.  [Hyperparameter Tuning with Keras Tuner](#2.-Hyperparameter-Tuning-with-Keras-Tuner)\n",
    "    * [2.1 Defining the Tunable Model Function (`build_lstm_model_`)](#2.1-Defining-the-Tunable-Model-Function-(build_lstm_model_))\n",
    "    * [2.2 Configuring and Running the Hyperband Tuner](#2.2-Configuring-and-Running-the-Hyperband-Tuner)\n",
    "    * [2.3 Analyzing Tuning Results](#2.3-Analyzing-Tuning-Results)\n",
    "3.  [Building and Evaluating the Final Model](#3.-Building-and-Evaluating-the-Final-Model)\n",
    "    * [3.1 Retrieving the Best Hyperparameters](#3.1-Retrieving-the-Best-Hyperparameters)\n",
    "    * [3.2 Training the Final Model with Optimal Hyperparameters](#3.2-Training-the-Final-Model-with-Optimal-Hyperparameters)\n",
    "    * [3.3 Saving the Final Model](#3.3-Saving-the-Final-Model)\n",
    "    * [3.4 Loading the True RUL for Test Data](#3.4-Loading-the-True-RUL-for-Test-Data)\n",
    "    * [3.5 Making Predictions with the Final Model](#3.5-Making-Predictions-with-the-Final-Model)\n",
    "    * [3.6 Quantitative Evaluation of the Final Model](#3.6-Quantitative-Evaluation-of-the-Final-Model)\n",
    "    * [3.7 Visualizing Final Model Performance](#3.7-Visualizing-Final-Model-Performance)\n",
    "4.  [Conclusion](#4.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f20ef9",
   "metadata": {
    "id": "d0f20ef9"
   },
   "source": [
    "## 1. Setup and Data Preparation\n",
    "\n",
    "This section handles the necessary imports and loads the preprocessed data, ensuring it's ready for model building and hyperparameter tuning.\n",
    "\n",
    "### 1.1 Importing Libraries and Loading Preprocessed Data\n",
    "\n",
    "We import core libraries like `tensorflow`, `numpy`, and `joblib` for model definition, data handling, and loading pre-saved scalers. The `X_train`, `y_train`, and `X_test` NumPy arrays, along with `rul_scaler` and `feature_scaler`, are loaded. These were generated in the `EDA.ipynb` notebook and represent the prepared time-series sequences and their corresponding RUL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89060559",
   "metadata": {
    "executionInfo": {
     "elapsed": 5310,
     "status": "ok",
     "timestamp": 1751871835051,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "89060559"
   },
   "outputs": [],
   "source": [
    "# Importing necessary modules for LSTM model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from custom_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2187b1",
   "metadata": {
    "executionInfo": {
     "elapsed": 28765,
     "status": "ok",
     "timestamp": 1751871863822,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "5b2187b1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Loading Datasets and scalers\n",
    "import joblib\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "\n",
    "rul_scaler = joblib.load('rul_scaler.pkl')\n",
    "scaler = joblib.load('feature_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4KvL4IHSvB",
   "metadata": {
    "id": "aa4KvL4IHSvB"
   },
   "source": [
    "### 3.4 Loading the True RUL for Test Data\n",
    "\n",
    "To evaluate the final model, we load the true Remaining Useful Life (RUL) values for the test engines from `RUL_FD001.txt`. These values serve as the ground truth for calculating evaluation metrics.\n",
    "\n",
    "The `rul_df` DataFrame is processed to isolate the RUL values and convert them into a NumPy array (`y_true`) for direct comparison with predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Xv-zRUUtHLeT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1751871864141,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "Xv-zRUUtHLeT",
    "outputId": "ed2b0083-c3b9-4d11-ae19-f68687a7ec03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[112]\n",
      " [ 98]\n",
      " [ 69]\n",
      " [ 82]\n",
      " [ 91]\n",
      " [ 93]\n",
      " [ 91]\n",
      " [ 95]\n",
      " [111]\n",
      " [ 96]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the true RUL for the test engines\n",
    "rul_df = pd.read_csv('test_rul/RUL_FD001.txt', sep=' ', header=None)\n",
    "rul_df.drop(columns=[1], inplace=True) # Drop the extra column that pandas creates\n",
    "rul_df.columns = ['RUL'] # Rename the column\n",
    "\n",
    "# Converting to array for evaluation metrics\n",
    "y_true = rul_df.to_numpy()\n",
    "print(y_true[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5YpXnM_rHBPX",
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1751871864181,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "5YpXnM_rHBPX"
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable() # Necessary to register the custom class in keras to avoid error during model loading\n",
    "# --- Custom PHM2008Score Metric Class ---\n",
    "class PHM2008Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='phm_2008_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_score = self.add_weight(name='total_score', initializer='zeros', dtype=tf.float64)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.int64)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true), tf.float32)\n",
    "        y_pred = tf.cast(tf.squeeze(y_pred), tf.float32)\n",
    "        d = y_pred - y_true\n",
    "        score_per_sample = tf.where(d < 0,\n",
    "                                    tf.exp(-d / 13.0) - 1,\n",
    "                                    tf.exp(d / 10.0) - 1)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(tf.squeeze(sample_weight), tf.float32)\n",
    "            score_per_sample = tf.multiply(score_per_sample, sample_weight)\n",
    "\n",
    "        self.total_score.assign_add(tf.reduce_sum(tf.cast(score_per_sample, tf.float64)))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.int64))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_score\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_score.assign(0.0)\n",
    "        self.num_samples.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e354f6d",
   "metadata": {
    "id": "3e354f6d"
   },
   "source": [
    "### 1.2 Creating Training and Validation Sets\n",
    "\n",
    "For robust hyperparameter tuning and model evaluation, it's crucial to split the training data (`X_train`, `y_train`) into separate training and validation sets. This allows Keras Tuner to evaluate different hyperparameter combinations on data it hasn't seen during training, preventing overfitting to the training set and providing a more realistic estimate of model performance.\n",
    "\n",
    "We use `train_test_split` from `sklearn.model_selection` to create these sets, reserving 20% of the data for validation and setting `random_state=42` for reproducibility.\n",
    "\n",
    "The output of `print(y_true[:10])` shows the first 10 true RUL values loaded from `RUL_FD001.txt`. These are the ground truth values we will compare our model's predictions against to assess its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f234c6f8",
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1751871864645,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "f234c6f8"
   },
   "outputs": [],
   "source": [
    "# Creating Seperate Training and Validation Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e202d4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1751871864664,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "7e202d4c",
    "outputId": "cf8504da-146c-47df-f50b-4c85072c67cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80197, 30, 61)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39612b18",
   "metadata": {
    "id": "39612b18"
   },
   "source": [
    "The output `(samples, timesteps, features)` confirms the shape of our training data, which is essential for defining the input layer of our neural network. For example, `X_train.shape[1]` provides the number of timesteps (sequence length), and `X_train.shape[2]` provides the number of features (sensor readings + operational settings) per timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5949f49",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1751871864683,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "c5949f49"
   },
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1],X_train.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e6210",
   "metadata": {
    "id": "ff8e6210"
   },
   "source": [
    "## 2. Hyperparameter Tuning with Keras Tuner\n",
    "\n",
    "This section details the core of our hyperparameter optimization process. We define a function that builds our LSTM model with various tunable parameters, then configure and execute Keras Tuner's Hyperband algorithm to efficiently search for the best combination of these parameters.\n",
    "\n",
    "### 2.1 Defining the Tunable Model Function (`build_lstm_model_`)\n",
    "\n",
    "The `build_lstm_model_` function is a crucial component for Keras Tuner. It takes a `HyperParameters` object (`hp`) as input, allowing us to define a search space for different architectural and training parameters.\n",
    "\n",
    "**Tunable Hyperparameters and Search Space:**\n",
    "\n",
    "* **`rnn_layer_type`**: Chooses between `'lstm'` and `'gru'` layers for the recurrent part of the network, allowing us to compare their effectiveness.\n",
    "* **`num_rnn_layers`**: Number of recurrent layers (LSTM or GRU) to stack, from 1 to 4.\n",
    "* **`{rnn_layer_type}_units_{i}`**: Number of units (neurons) in each recurrent layer, ranging from 32 to 512, in steps of 32.\n",
    "* **`use_bidirectional_{i}`**: A boolean choice to use a Bidirectional wrapper around each RNN layer, potentially capturing dependencies in both forward and backward directions.\n",
    "* **`use_recurrent_dropout_{i}`**: A boolean to enable recurrent dropout within the RNN layer to combat overfitting.\n",
    "* **`recurrent_dropout_rate_{i}`**: Dropout rate for recurrent connections, from 0.0 to 0.3, in steps of 0.05, if `use_recurrent_dropout` is true.\n",
    "* **`dropout_rnn_{i}`**: Standard dropout layer applied after each RNN layer, with a rate from 0.1 to 0.5.\n",
    "* **`num_dense_layers`**: Number of Dense (fully connected) layers after the RNN layers, from 0 to 3.\n",
    "* **`dense_units_{j}`**: Number of units in each Dense layer, from 32 to 256, in steps of 32.\n",
    "* **`dropout_dense_{j}`**: Standard dropout layer after each Dense layer, with a rate from 0.1 to 0.5.\n",
    "* **`activation`**: Activation function for Dense layers (excluding the output layer), choosing between `'relu'`, `'leaky_relu'`, and `'elu'`.\n",
    "* **`learning_rate`**: Learning rate for the optimizer, selected from a discrete set of values (e.g., 0.01, 0.001, 0.0001).\n",
    "* **`optimizer`**: Choice of optimizer: `'adam'`, `'rmsprop'`, or `'sgd'`.\n",
    "* **`loss`**: Choice of loss function: `'mse'` (Mean Squared Error) or `'huber'`. Huber loss is less sensitive to outliers than MSE.\n",
    "* **`l1_regularization_factor`, `l2_regularization_factor`**: L1 and L2 regularization for kernel weights, ranging from 1e-5 to 1e-2.\n",
    "\n",
    "This extensive search space allows for a thorough exploration of model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5b4a73",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1751871864726,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "0a5b4a73"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import LeakyReLU # Import LeakyReLU\n",
    "from tensorflow.keras.losses import Huber # Import Huber loss\n",
    "\n",
    "def build_lstm_model_(hp):\n",
    "    \"\"\"\n",
    "    Builds a Keras Sequential model with LSTM or GRU layers,\n",
    "    tunable hyperparameters using Keras Tuner.\n",
    "\n",
    "    Args:\n",
    "        hp: Keras Tuner HyperParameters object.\n",
    "        input_shape: A tuple representing the input shape (timesteps, features).\n",
    "                     e.g., (X_train.shape[1], X_train.shape[2])\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer shape\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "\n",
    "    # --- Feature: Allow choice between LSTM and GRU ---\n",
    "    rnn_layer_type = hp.Choice('rnn_layer_type', values=['lstm', 'gru'])\n",
    "\n",
    "    # --- Enhanced LSTM/GRU Layer Configuration ---\n",
    "    num_rnn_layers = hp.Int('num_rnn_layers', min_value=1, max_value=4, step=1)\n",
    "\n",
    "    for i in range(num_rnn_layers):\n",
    "        # Units for RNN layers\n",
    "        rnn_units = hp.Int(f'{rnn_layer_type}_units_{i}', min_value=32, max_value=512, step=32)\n",
    "\n",
    "        # Optional: Bidirectional wrapper\n",
    "        use_bidirectional = hp.Boolean(f'use_bidirectional_{i}')\n",
    "        use_recurrent_dropout = hp.Boolean(f'use_recurrent_dropout_{i}')\n",
    "        recurrent_dropout_rate = hp.Float(f'recurrent_dropout_rate_{i}', min_value=0.0, max_value=0.3, step=0.05) if use_recurrent_dropout else 0.0\n",
    "\n",
    "        # Determine if the layer should return sequences\n",
    "        return_sequences = (i < num_rnn_layers - 1)\n",
    "\n",
    "        if rnn_layer_type == 'lstm':\n",
    "            rnn_layer = LSTM(\n",
    "                units=rnn_units,\n",
    "                return_sequences=return_sequences,\n",
    "                recurrent_dropout=recurrent_dropout_rate\n",
    "            )\n",
    "        else: # GRU\n",
    "            rnn_layer = GRU(\n",
    "                units=rnn_units,\n",
    "                return_sequences=return_sequences,\n",
    "                recurrent_dropout=recurrent_dropout_rate\n",
    "            )\n",
    "\n",
    "        if use_bidirectional:\n",
    "            model.add(Bidirectional(rnn_layer))\n",
    "        else:\n",
    "            model.add(rnn_layer)\n",
    "\n",
    "        # Standard Dropout after RNN layer\n",
    "        if hp.Boolean(f'dropout_rnn_{i}'):\n",
    "            model.add(Dropout(rate=hp.Float(f'dropout_rate_rnn_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # --- Feature: Multiple Dense Layers ---\n",
    "    num_dense_layers = hp.Int('num_dense_layers', min_value=0, max_value=2, step=1)\n",
    "\n",
    "    for i in range(num_dense_layers):\n",
    "        dense_activation_choice = hp.Choice(f'dense_activation_{i}', values=['relu', 'tanh', 'leaky_relu'])\n",
    "        activation_function = None\n",
    "        if dense_activation_choice == 'leaky_relu':\n",
    "            activation_function = LeakyReLU() # Use LeakyReLU instance\n",
    "        else:\n",
    "            activation_function = dense_activation_choice # Use string for 'relu' or 'tanh'\n",
    "\n",
    "        # Regularization for Dense layers\n",
    "        l1_reg = hp.Float(f'l1_reg_dense_{i}', min_value=1e-5, max_value=1e-2, sampling='log') if hp.Boolean(f'use_l1_reg_dense_{i}') else 0.0\n",
    "        l2_reg = hp.Float(f'l2_reg_dense_{i}', min_value=1e-5, max_value=1e-2, sampling='log') if hp.Boolean(f'use_l2_reg_dense_{i}') else 0.0\n",
    "\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(f'dense_units_{i}', min_value=32, max_value=256, step=32),\n",
    "            activation=activation_function,\n",
    "            kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)\n",
    "        ))\n",
    "        if hp.Boolean(f'dropout_dense_{i}'):\n",
    "            model.add(Dropout(rate=hp.Float(f'dropout_rate_dense_{i}', min_value=0.1, max_value=0.6, step=0.1)))\n",
    "\n",
    "    # Output layer (1 unit for regression)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # --- Enhanced Optimizer and Learning Rate ---\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else: # sgd\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    loss_function_choice = hp.Choice('loss_function', values=['mse', 'mae', 'huber_loss'])\n",
    "    loss_function = None\n",
    "    if loss_function_choice == 'huber_loss':\n",
    "        loss_function = Huber() # Use Huber instance\n",
    "    else:\n",
    "        loss_function = loss_function_choice # Use string for 'mse' or 'mae'\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function,\n",
    "        metrics=['mae', PHM2008Score()] # MAE for interpretability, PHM2008Score for objective\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ffc42",
   "metadata": {
    "id": "1d0ffc42"
   },
   "source": [
    "### 2.2 Configuring and Running the Hyperband Tuner\n",
    "\n",
    "We initialize and run the Keras Tuner's `Hyperband` algorithm. Hyperband is an efficient hyperparameter optimization method that adaptively allocates resources (training epochs) to different hyperparameter configurations, quickly discarding poorly performing ones.\n",
    "\n",
    "**Tuner Configuration:**\n",
    "\n",
    "* **`hypermodel=build_lstm_model_`**: The function that creates the Keras model with tunable hyperparameters.\n",
    "* **`objective='val_loss'`**: The metric to monitor and minimize during the search (validation loss).\n",
    "* **`max_epochs=50`**: The maximum number of epochs to train any given model candidate.\n",
    "* **`factor=3`**: The reduction factor for the number of models and epochs in successive halving.\n",
    "* **`directory='my_dir'`**: Directory to store tuning results.\n",
    "* **`project_name='intro_to_kt'`**: Name of the sub-directory within `directory`.\n",
    "\n",
    "The `tuner.search()` method then executes the hyperparameter search, training different model configurations on the `X_train` and `y_train` data, while validating on `X_val` and `y_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c342751c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 97064,
     "status": "error",
     "timestamp": 1751872422331,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "c342751c",
    "outputId": "edddf655-a2ca-469a-83ba-0f2cc1059164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from _lstm_tuning_dir/lstm_hyperband_tuning/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 50\n",
      "rnn_layer_type (Choice)\n",
      "{'default': 'lstm', 'conditions': [], 'values': ['lstm', 'gru'], 'ordered': False}\n",
      "num_rnn_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
      "lstm_units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_bidirectional_0 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "use_recurrent_dropout_0 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout_rnn_0 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "num_dense_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 0, 'max_value': 2, 'step': 1, 'sampling': 'linear'}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'rmsprop', 'sgd'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "loss_function (Choice)\n",
      "{'default': 'mse', 'conditions': [], 'values': ['mse', 'mae', 'huber_loss'], 'ordered': False}\n",
      "gru_units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "dropout_rate_rnn_0 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "gru_units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_bidirectional_1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "use_recurrent_dropout_1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout_rnn_1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lstm_units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "lstm_units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_bidirectional_2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "use_recurrent_dropout_2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout_rnn_2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "recurrent_dropout_rate_1 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': 0.05, 'sampling': 'linear'}\n",
      "dropout_rate_rnn_1 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "recurrent_dropout_rate_2 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': 0.05, 'sampling': 'linear'}\n",
      "dropout_rate_rnn_2 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "dense_activation_0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'leaky_relu'], 'ordered': False}\n",
      "use_l1_reg_dense_0 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "use_l2_reg_dense_0 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dense_units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "dropout_dense_0 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "recurrent_dropout_rate_0 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': 0.05, 'sampling': 'linear'}\n",
      "gru_units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "gru_units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_bidirectional_3 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "use_recurrent_dropout_3 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout_rnn_3 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "recurrent_dropout_rate_3 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': 0.05, 'sampling': 'linear'}\n",
      "dense_activation_1 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'leaky_relu'], 'ordered': False}\n",
      "use_l1_reg_dense_1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "use_l2_reg_dense_1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dense_units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "dropout_dense_1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lstm_units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "dropout_rate_rnn_3 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "l1_reg_dense_0 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "dropout_rate_dense_0 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.6, 'step': 0.1, 'sampling': 'linear'}\n",
      "l1_reg_dense_1 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "l2_reg_dense_1 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "l2_reg_dense_0 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "dropout_rate_dense_1 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.6, 'step': 0.1, 'sampling': 'linear'}\n",
      "\n",
      "Starting hyperparameter search...\n",
      "\n",
      "Hyperparameter search complete!\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "# --- Initialize the Tuner ---\n",
    "tuner = kt.Hyperband(\n",
    "    build_lstm_model_,\n",
    "    objective=kt.Objective('val_phm_2008_score', direction='min'), # Keras Tuner looks for 'val_' prefix\n",
    "    max_epochs=50,                     # Max epochs to train a model in Hyperband\n",
    "    factor=3,                          # Reduction factor for Hyperband\n",
    "    directory='_lstm_tuning_dir',    # Directory to store results\n",
    "    project_name='lstm_hyperband_tuning', # Name of the project\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "# Print a summary of the search space\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# --- Run the Search ---\n",
    "# Create a callback to stop training when a metric has stopped improving\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_phm_2008_score', patience=10, mode='min', restore_best_weights=True)\n",
    "\n",
    "print(\"\\nStarting hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train,y_train, # Pass the tf.data.Dataset here\n",
    "    validation_data=(X_val,y_val), # Pass the tf.data.Dataset here\n",
    "    callbacks=[stop_early],# Use the early stopping callback\n",
    "    batch_size=2048,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nHyperparameter search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e601c3",
   "metadata": {
    "id": "d7e601c3"
   },
   "source": [
    "## 3. Building and Evaluating the Final Model\n",
    "\n",
    "Once the hyperparameter tuning is complete, we retrieve the best hyperparameters found by the tuner and use them to train a final model on the *entire* training dataset (X_train, y_train, including the validation split that was used by the tuner). This final model is then evaluated on the completely unseen test data.\n",
    "\n",
    "### 3.1 Retrieving the Best Hyperparameters\n",
    "\n",
    "We extract the `best_hps` object from the tuner, which contains the optimal set of hyperparameters that resulted in the lowest validation loss during the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39303bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Best Hyperparameters Found ---\n",
      "  Optimizer: adam\n",
      "  Learning Rate: 0.000436\n",
      "  Loss Function: mae\n",
      "\n",
      "  Best RNN Layer Type: GRU\n",
      "  Number of RNN Layers: 4\n",
      "\n",
      "  --- RNN Layer 1 ---\n",
      "    Units: 416\n",
      "    Bidirectional: False\n",
      "    Recurrent Dropout Rate: 0.15\n",
      "    Standard Dropout Rate (after RNN): 0.50\n",
      "\n",
      "  --- RNN Layer 2 ---\n",
      "    Units: 64\n",
      "    Bidirectional: True\n",
      "    Recurrent Dropout Rate: 0.05\n",
      "    Standard Dropout Rate (after RNN): 0.20\n",
      "\n",
      "  --- RNN Layer 3 ---\n",
      "    Units: 128\n",
      "    Bidirectional: True\n",
      "    Recurrent Dropout: Not used\n",
      "    Standard Dropout Rate (after RNN): 0.20\n",
      "\n",
      "  --- RNN Layer 4 ---\n",
      "    Units: 288\n",
      "    Bidirectional: False\n",
      "    Recurrent Dropout: Not used\n",
      "    Standard Dropout Rate (after RNN): 0.50\n",
      "\n",
      "  Number of Dense Layers: 0\n",
      "\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751891852.781231     720 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2246 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of the best model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/play_box/.venvs/rapids_python3.12/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">597,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">185,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">198,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">471,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m416\u001b[0m)        │       \u001b[38;5;34m597,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m416\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m185,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m198,144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m)            │       \u001b[38;5;34m471,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m289\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,453,057</span> (5.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,453,057\u001b[0m (5.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,453,057</span> (5.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,453,057\u001b[0m (5.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in _lstm_tuning_dir/lstm_hyperband_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_phm_2008_score\", direction=\"min\")\n",
      "\n",
      "Trial 0050 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: gru\n",
      "num_rnn_layers: 4\n",
      "lstm_units_0: 224\n",
      "use_bidirectional_0: False\n",
      "use_recurrent_dropout_0: True\n",
      "dropout_rnn_0: True\n",
      "num_dense_layers: 0\n",
      "optimizer: adam\n",
      "learning_rate: 0.0004357443516879724\n",
      "loss_function: mae\n",
      "gru_units_0: 416\n",
      "dropout_rate_rnn_0: 0.5\n",
      "gru_units_1: 64\n",
      "use_bidirectional_1: True\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: True\n",
      "lstm_units_1: 160\n",
      "lstm_units_2: 160\n",
      "use_bidirectional_2: True\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: True\n",
      "recurrent_dropout_rate_1: 0.05\n",
      "dropout_rate_rnn_1: 0.2\n",
      "recurrent_dropout_rate_2: 0.15000000000000002\n",
      "dropout_rate_rnn_2: 0.2\n",
      "dense_activation_0: tanh\n",
      "use_l1_reg_dense_0: True\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 224\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.15000000000000002\n",
      "gru_units_2: 128\n",
      "gru_units_3: 288\n",
      "use_bidirectional_3: False\n",
      "use_recurrent_dropout_3: False\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.05\n",
      "dense_activation_1: leaky_relu\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: True\n",
      "dense_units_1: 256\n",
      "dropout_dense_1: True\n",
      "lstm_units_3: 448\n",
      "dropout_rate_rnn_3: 0.5\n",
      "l1_reg_dense_0: 1.406322023448604e-05\n",
      "dropout_rate_dense_0: 0.4\n",
      "l1_reg_dense_1: 0.0003763845443899242\n",
      "l2_reg_dense_1: 0.004587859230249849\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0046\n",
      "l2_reg_dense_0: 4.312673386190453e-05\n",
      "dropout_rate_dense_1: 0.5\n",
      "Score: 96.00660526752472\n",
      "\n",
      "Trial 0082 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: lstm\n",
      "num_rnn_layers: 2\n",
      "lstm_units_0: 32\n",
      "use_bidirectional_0: True\n",
      "use_recurrent_dropout_0: False\n",
      "dropout_rnn_0: False\n",
      "num_dense_layers: 0\n",
      "optimizer: adam\n",
      "learning_rate: 0.0004018083459056581\n",
      "loss_function: huber_loss\n",
      "gru_units_0: 384\n",
      "dropout_rate_rnn_0: 0.5\n",
      "gru_units_1: 384\n",
      "use_bidirectional_1: True\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: True\n",
      "lstm_units_1: 32\n",
      "lstm_units_2: 320\n",
      "use_bidirectional_2: True\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: False\n",
      "recurrent_dropout_rate_1: 0.15000000000000002\n",
      "dropout_rate_rnn_1: 0.30000000000000004\n",
      "recurrent_dropout_rate_2: 0.0\n",
      "dropout_rate_rnn_2: 0.1\n",
      "dense_activation_0: relu\n",
      "use_l1_reg_dense_0: True\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 64\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.15000000000000002\n",
      "gru_units_2: 128\n",
      "gru_units_3: 32\n",
      "use_bidirectional_3: True\n",
      "use_recurrent_dropout_3: True\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.1\n",
      "dense_activation_1: tanh\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: True\n",
      "dense_units_1: 224\n",
      "dropout_dense_1: False\n",
      "lstm_units_3: 448\n",
      "dropout_rate_rnn_3: 0.30000000000000004\n",
      "l1_reg_dense_0: 5.036800270367776e-05\n",
      "dropout_rate_dense_0: 0.2\n",
      "l1_reg_dense_1: 0.0001622395833503731\n",
      "l2_reg_dense_1: 2.013574524253807e-05\n",
      "l2_reg_dense_0: 0.001324175700942734\n",
      "dropout_rate_dense_1: 0.5\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0080\n",
      "Score: 103.47618985176086\n",
      "\n",
      "Trial 0046 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: gru\n",
      "num_rnn_layers: 4\n",
      "lstm_units_0: 224\n",
      "use_bidirectional_0: False\n",
      "use_recurrent_dropout_0: True\n",
      "dropout_rnn_0: True\n",
      "num_dense_layers: 0\n",
      "optimizer: adam\n",
      "learning_rate: 0.0004357443516879724\n",
      "loss_function: mae\n",
      "gru_units_0: 416\n",
      "dropout_rate_rnn_0: 0.5\n",
      "gru_units_1: 64\n",
      "use_bidirectional_1: True\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: True\n",
      "lstm_units_1: 160\n",
      "lstm_units_2: 160\n",
      "use_bidirectional_2: True\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: True\n",
      "recurrent_dropout_rate_1: 0.05\n",
      "dropout_rate_rnn_1: 0.2\n",
      "recurrent_dropout_rate_2: 0.15000000000000002\n",
      "dropout_rate_rnn_2: 0.2\n",
      "dense_activation_0: tanh\n",
      "use_l1_reg_dense_0: True\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 224\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.15000000000000002\n",
      "gru_units_2: 128\n",
      "gru_units_3: 288\n",
      "use_bidirectional_3: False\n",
      "use_recurrent_dropout_3: False\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.05\n",
      "dense_activation_1: leaky_relu\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: True\n",
      "dense_units_1: 256\n",
      "dropout_dense_1: True\n",
      "lstm_units_3: 448\n",
      "dropout_rate_rnn_3: 0.5\n",
      "l1_reg_dense_0: 1.406322023448604e-05\n",
      "dropout_rate_dense_0: 0.4\n",
      "l1_reg_dense_1: 0.0003763845443899242\n",
      "l2_reg_dense_1: 0.004587859230249849\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0034\n",
      "l2_reg_dense_0: 4.312673386190453e-05\n",
      "dropout_rate_dense_1: 0.5\n",
      "Score: 104.55837202072144\n",
      "\n",
      "Trial 0073 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: gru\n",
      "num_rnn_layers: 1\n",
      "lstm_units_0: 448\n",
      "use_bidirectional_0: False\n",
      "use_recurrent_dropout_0: True\n",
      "dropout_rnn_0: False\n",
      "num_dense_layers: 2\n",
      "optimizer: adam\n",
      "learning_rate: 1.3270473096227068e-05\n",
      "loss_function: mae\n",
      "gru_units_0: 512\n",
      "dropout_rate_rnn_0: 0.1\n",
      "gru_units_1: 320\n",
      "use_bidirectional_1: True\n",
      "use_recurrent_dropout_1: False\n",
      "dropout_rnn_1: False\n",
      "lstm_units_1: 512\n",
      "lstm_units_2: 288\n",
      "use_bidirectional_2: True\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: False\n",
      "recurrent_dropout_rate_1: 0.15000000000000002\n",
      "dropout_rate_rnn_1: 0.4\n",
      "recurrent_dropout_rate_2: 0.1\n",
      "dropout_rate_rnn_2: 0.2\n",
      "dense_activation_0: relu\n",
      "use_l1_reg_dense_0: False\n",
      "use_l2_reg_dense_0: True\n",
      "dense_units_0: 128\n",
      "dropout_dense_0: False\n",
      "recurrent_dropout_rate_0: 0.15000000000000002\n",
      "gru_units_2: 160\n",
      "gru_units_3: 320\n",
      "use_bidirectional_3: False\n",
      "use_recurrent_dropout_3: True\n",
      "dropout_rnn_3: False\n",
      "recurrent_dropout_rate_3: 0.0\n",
      "dense_activation_1: relu\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: True\n",
      "dense_units_1: 128\n",
      "dropout_dense_1: False\n",
      "lstm_units_3: 352\n",
      "dropout_rate_rnn_3: 0.30000000000000004\n",
      "l1_reg_dense_0: 0.0005247660529339657\n",
      "dropout_rate_dense_0: 0.5\n",
      "l1_reg_dense_1: 0.0016026806059106574\n",
      "l2_reg_dense_1: 8.307700751515348e-05\n",
      "l2_reg_dense_0: 0.00016952572690285674\n",
      "dropout_rate_dense_1: 0.4\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0071\n",
      "Score: 105.21293938159943\n",
      "\n",
      "Trial 0083 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: gru\n",
      "num_rnn_layers: 4\n",
      "lstm_units_0: 352\n",
      "use_bidirectional_0: False\n",
      "use_recurrent_dropout_0: True\n",
      "dropout_rnn_0: True\n",
      "num_dense_layers: 0\n",
      "optimizer: adam\n",
      "learning_rate: 2.5901137727809492e-05\n",
      "loss_function: huber_loss\n",
      "gru_units_0: 416\n",
      "dropout_rate_rnn_0: 0.2\n",
      "gru_units_1: 160\n",
      "use_bidirectional_1: False\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: True\n",
      "lstm_units_1: 288\n",
      "lstm_units_2: 192\n",
      "use_bidirectional_2: True\n",
      "use_recurrent_dropout_2: True\n",
      "dropout_rnn_2: True\n",
      "recurrent_dropout_rate_1: 0.0\n",
      "dropout_rate_rnn_1: 0.30000000000000004\n",
      "recurrent_dropout_rate_2: 0.0\n",
      "dropout_rate_rnn_2: 0.1\n",
      "dense_activation_0: tanh\n",
      "use_l1_reg_dense_0: True\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 256\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.0\n",
      "gru_units_2: 352\n",
      "gru_units_3: 32\n",
      "use_bidirectional_3: True\n",
      "use_recurrent_dropout_3: True\n",
      "dropout_rnn_3: False\n",
      "recurrent_dropout_rate_3: 0.0\n",
      "dense_activation_1: relu\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: False\n",
      "dense_units_1: 128\n",
      "dropout_dense_1: True\n",
      "lstm_units_3: 288\n",
      "dropout_rate_rnn_3: 0.5\n",
      "l1_reg_dense_0: 0.00023342577271778504\n",
      "dropout_rate_dense_0: 0.2\n",
      "l1_reg_dense_1: 4.9854710419866835e-05\n",
      "l2_reg_dense_1: 0.002170732485910571\n",
      "l2_reg_dense_0: 0.0001920157562139474\n",
      "dropout_rate_dense_1: 0.4\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0081\n",
      "Score: 106.38089537620544\n",
      "\n",
      "Trial 0051 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: lstm\n",
      "num_rnn_layers: 4\n",
      "lstm_units_0: 256\n",
      "use_bidirectional_0: True\n",
      "use_recurrent_dropout_0: True\n",
      "dropout_rnn_0: False\n",
      "num_dense_layers: 1\n",
      "optimizer: adam\n",
      "learning_rate: 9.511822743678012e-05\n",
      "loss_function: huber_loss\n",
      "gru_units_0: 352\n",
      "dropout_rate_rnn_0: 0.2\n",
      "gru_units_1: 96\n",
      "use_bidirectional_1: False\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: False\n",
      "lstm_units_1: 448\n",
      "lstm_units_2: 96\n",
      "use_bidirectional_2: False\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: False\n",
      "recurrent_dropout_rate_1: 0.05\n",
      "dropout_rate_rnn_1: 0.5\n",
      "recurrent_dropout_rate_2: 0.1\n",
      "dropout_rate_rnn_2: 0.2\n",
      "dense_activation_0: leaky_relu\n",
      "use_l1_reg_dense_0: False\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 64\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.25\n",
      "gru_units_2: 32\n",
      "gru_units_3: 512\n",
      "use_bidirectional_3: True\n",
      "use_recurrent_dropout_3: False\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.25\n",
      "dense_activation_1: leaky_relu\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: False\n",
      "dense_units_1: 256\n",
      "dropout_dense_1: False\n",
      "lstm_units_3: 128\n",
      "dropout_rate_rnn_3: 0.5\n",
      "l1_reg_dense_0: 5.1554992326239796e-05\n",
      "dropout_rate_dense_0: 0.5\n",
      "l1_reg_dense_1: 0.0030989813757385442\n",
      "l2_reg_dense_1: 0.00213898563809781\n",
      "l2_reg_dense_0: 0.00041138828537009723\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0049\n",
      "dropout_rate_dense_1: 0.2\n",
      "Score: 106.82381403446198\n",
      "\n",
      "Trial 0072 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: lstm\n",
      "num_rnn_layers: 1\n",
      "lstm_units_0: 32\n",
      "use_bidirectional_0: False\n",
      "use_recurrent_dropout_0: False\n",
      "dropout_rnn_0: True\n",
      "num_dense_layers: 0\n",
      "optimizer: adam\n",
      "learning_rate: 0.00017964270663470668\n",
      "loss_function: huber_loss\n",
      "gru_units_0: 64\n",
      "dropout_rate_rnn_0: 0.5\n",
      "gru_units_1: 480\n",
      "use_bidirectional_1: True\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: False\n",
      "lstm_units_1: 352\n",
      "lstm_units_2: 192\n",
      "use_bidirectional_2: True\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: True\n",
      "recurrent_dropout_rate_1: 0.2\n",
      "dropout_rate_rnn_1: 0.1\n",
      "recurrent_dropout_rate_2: 0.2\n",
      "dropout_rate_rnn_2: 0.2\n",
      "dense_activation_0: leaky_relu\n",
      "use_l1_reg_dense_0: False\n",
      "use_l2_reg_dense_0: True\n",
      "dense_units_0: 64\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.05\n",
      "gru_units_2: 448\n",
      "gru_units_3: 160\n",
      "use_bidirectional_3: False\n",
      "use_recurrent_dropout_3: True\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.15000000000000002\n",
      "dense_activation_1: leaky_relu\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: True\n",
      "dense_units_1: 96\n",
      "dropout_dense_1: False\n",
      "lstm_units_3: 224\n",
      "dropout_rate_rnn_3: 0.5\n",
      "l1_reg_dense_0: 0.00014007065909395384\n",
      "dropout_rate_dense_0: 0.30000000000000004\n",
      "l1_reg_dense_1: 0.0018504492091231128\n",
      "l2_reg_dense_1: 0.002346900712835023\n",
      "l2_reg_dense_0: 1.3858123680639039e-05\n",
      "dropout_rate_dense_1: 0.5\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0070\n",
      "Score: 107.84406173229218\n",
      "\n",
      "Trial 0080 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: lstm\n",
      "num_rnn_layers: 2\n",
      "lstm_units_0: 32\n",
      "use_bidirectional_0: True\n",
      "use_recurrent_dropout_0: False\n",
      "dropout_rnn_0: False\n",
      "num_dense_layers: 0\n",
      "optimizer: adam\n",
      "learning_rate: 0.0004018083459056581\n",
      "loss_function: huber_loss\n",
      "gru_units_0: 384\n",
      "dropout_rate_rnn_0: 0.5\n",
      "gru_units_1: 384\n",
      "use_bidirectional_1: True\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: True\n",
      "lstm_units_1: 32\n",
      "lstm_units_2: 320\n",
      "use_bidirectional_2: True\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: False\n",
      "recurrent_dropout_rate_1: 0.15000000000000002\n",
      "dropout_rate_rnn_1: 0.30000000000000004\n",
      "recurrent_dropout_rate_2: 0.0\n",
      "dropout_rate_rnn_2: 0.1\n",
      "dense_activation_0: relu\n",
      "use_l1_reg_dense_0: True\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 64\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.15000000000000002\n",
      "gru_units_2: 128\n",
      "gru_units_3: 32\n",
      "use_bidirectional_3: True\n",
      "use_recurrent_dropout_3: True\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.1\n",
      "dense_activation_1: tanh\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: True\n",
      "dense_units_1: 224\n",
      "dropout_dense_1: False\n",
      "lstm_units_3: 448\n",
      "dropout_rate_rnn_3: 0.30000000000000004\n",
      "l1_reg_dense_0: 5.036800270367776e-05\n",
      "dropout_rate_dense_0: 0.2\n",
      "l1_reg_dense_1: 0.0001622395833503731\n",
      "l2_reg_dense_1: 2.013574524253807e-05\n",
      "l2_reg_dense_0: 0.001324175700942734\n",
      "dropout_rate_dense_1: 0.5\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 109.0795806646347\n",
      "\n",
      "Trial 0049 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: lstm\n",
      "num_rnn_layers: 4\n",
      "lstm_units_0: 256\n",
      "use_bidirectional_0: True\n",
      "use_recurrent_dropout_0: True\n",
      "dropout_rnn_0: False\n",
      "num_dense_layers: 1\n",
      "optimizer: adam\n",
      "learning_rate: 9.511822743678012e-05\n",
      "loss_function: huber_loss\n",
      "gru_units_0: 352\n",
      "dropout_rate_rnn_0: 0.2\n",
      "gru_units_1: 96\n",
      "use_bidirectional_1: False\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: False\n",
      "lstm_units_1: 448\n",
      "lstm_units_2: 96\n",
      "use_bidirectional_2: False\n",
      "use_recurrent_dropout_2: False\n",
      "dropout_rnn_2: False\n",
      "recurrent_dropout_rate_1: 0.05\n",
      "dropout_rate_rnn_1: 0.5\n",
      "recurrent_dropout_rate_2: 0.1\n",
      "dropout_rate_rnn_2: 0.2\n",
      "dense_activation_0: leaky_relu\n",
      "use_l1_reg_dense_0: False\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 64\n",
      "dropout_dense_0: True\n",
      "recurrent_dropout_rate_0: 0.25\n",
      "gru_units_2: 32\n",
      "gru_units_3: 512\n",
      "use_bidirectional_3: True\n",
      "use_recurrent_dropout_3: False\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.25\n",
      "dense_activation_1: leaky_relu\n",
      "use_l1_reg_dense_1: True\n",
      "use_l2_reg_dense_1: False\n",
      "dense_units_1: 256\n",
      "dropout_dense_1: False\n",
      "lstm_units_3: 128\n",
      "dropout_rate_rnn_3: 0.5\n",
      "l1_reg_dense_0: 5.1554992326239796e-05\n",
      "dropout_rate_dense_0: 0.5\n",
      "l1_reg_dense_1: 0.0030989813757385442\n",
      "l2_reg_dense_1: 0.00213898563809781\n",
      "l2_reg_dense_0: 0.00041138828537009723\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0037\n",
      "dropout_rate_dense_1: 0.2\n",
      "Score: 109.69253766536713\n",
      "\n",
      "Trial 0047 summary\n",
      "Hyperparameters:\n",
      "rnn_layer_type: lstm\n",
      "num_rnn_layers: 3\n",
      "lstm_units_0: 512\n",
      "use_bidirectional_0: False\n",
      "use_recurrent_dropout_0: False\n",
      "dropout_rnn_0: False\n",
      "num_dense_layers: 1\n",
      "optimizer: adam\n",
      "learning_rate: 5.1217820838695444e-05\n",
      "loss_function: mse\n",
      "gru_units_0: 352\n",
      "dropout_rate_rnn_0: 0.5\n",
      "gru_units_1: 32\n",
      "use_bidirectional_1: False\n",
      "use_recurrent_dropout_1: True\n",
      "dropout_rnn_1: True\n",
      "lstm_units_1: 416\n",
      "lstm_units_2: 384\n",
      "use_bidirectional_2: False\n",
      "use_recurrent_dropout_2: True\n",
      "dropout_rnn_2: True\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "recurrent_dropout_rate_1: 0.0\n",
      "dropout_rate_rnn_1: 0.1\n",
      "recurrent_dropout_rate_2: 0.0\n",
      "dropout_rate_rnn_2: 0.1\n",
      "dense_activation_0: relu\n",
      "use_l1_reg_dense_0: False\n",
      "use_l2_reg_dense_0: False\n",
      "dense_units_0: 32\n",
      "dropout_dense_0: False\n",
      "tuner/trial_id: 0040\n",
      "recurrent_dropout_rate_0: 0.05\n",
      "gru_units_2: 160\n",
      "gru_units_3: 352\n",
      "use_bidirectional_3: False\n",
      "use_recurrent_dropout_3: False\n",
      "dropout_rnn_3: True\n",
      "recurrent_dropout_rate_3: 0.1\n",
      "dense_activation_1: relu\n",
      "use_l1_reg_dense_1: False\n",
      "use_l2_reg_dense_1: False\n",
      "dense_units_1: 160\n",
      "dropout_dense_1: True\n",
      "lstm_units_3: 224\n",
      "dropout_rate_rnn_3: 0.2\n",
      "l1_reg_dense_0: 3.9642867417415074e-05\n",
      "dropout_rate_dense_0: 0.1\n",
      "l1_reg_dense_1: 2.1385537359043936e-05\n",
      "l2_reg_dense_1: 1.6619592083179487e-05\n",
      "l2_reg_dense_0: 1.795543650636074e-05\n",
      "dropout_rate_dense_1: 0.30000000000000004\n",
      "Score: 109.80096185207367\n"
     ]
    }
   ],
   "source": [
    "# --- Get the best hyperparameters ---\n",
    "# Ensure 'tuner' object is initialized and has loaded previous results.\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "\n",
    "# Access values directly from best_hps.values dictionary\n",
    "hp_values = best_hps.values\n",
    "\n",
    "# General hyperparameters\n",
    "best_optimizer = hp_values.get('optimizer')\n",
    "best_learning_rate = hp_values.get('learning_rate')\n",
    "best_loss_function = hp_values.get('loss_function')\n",
    "\n",
    "print(f\"  Optimizer: {best_optimizer}\")\n",
    "print(f\"  Learning Rate: {best_learning_rate:.6f}\")\n",
    "print(f\"  Loss Function: {best_loss_function}\")\n",
    "\n",
    "# --- RNN Layer Configuration ---\n",
    "best_rnn_layer_type = hp_values.get('rnn_layer_type')\n",
    "best_num_rnn_layers = hp_values.get('num_rnn_layers')\n",
    "\n",
    "print(f\"\\n  Best RNN Layer Type: {best_rnn_layer_type.upper()}\")\n",
    "print(f\"  Number of RNN Layers: {best_num_rnn_layers}\")\n",
    "\n",
    "for i in range(best_num_rnn_layers):\n",
    "    print(f\"\\n  --- RNN Layer {i+1} ---\")\n",
    "    best_rnn_units = hp_values.get(f'{best_rnn_layer_type}_units_{i}')\n",
    "    print(f\"    Units: {best_rnn_units}\")\n",
    "\n",
    "    # Access boolean hyperparameters and associated rates\n",
    "    use_bidirectional_key = f'use_bidirectional_{i}'\n",
    "    if hp_values.get(use_bidirectional_key, False): # Using dict.get() with default\n",
    "        print(f\"    Bidirectional: True\")\n",
    "    else:\n",
    "        print(f\"    Bidirectional: False\")\n",
    "\n",
    "    use_recurrent_dropout_key = f'use_recurrent_dropout_{i}'\n",
    "    if hp_values.get(use_recurrent_dropout_key, False):\n",
    "        best_recurrent_dropout_rate = hp_values.get(f'recurrent_dropout_rate_{i}')\n",
    "        print(f\"    Recurrent Dropout Rate: {best_recurrent_dropout_rate:.2f}\")\n",
    "    else:\n",
    "        print(f\"    Recurrent Dropout: Not used\")\n",
    "\n",
    "\n",
    "    dropout_rnn_key = f'dropout_rnn_{i}'\n",
    "    if hp_values.get(dropout_rnn_key, False):\n",
    "        best_dropout_rate_rnn = hp_values.get(f'dropout_rate_rnn_{i}')\n",
    "        print(f\"    Standard Dropout Rate (after RNN): {best_dropout_rate_rnn:.2f}\")\n",
    "    else:\n",
    "        print(f\"    Standard Dropout (after RNN): Not used\")\n",
    "\n",
    "\n",
    "# --- Dense Layer Configuration ---\n",
    "best_num_dense_layers = hp_values.get('num_dense_layers')\n",
    "print(f\"\\n  Number of Dense Layers: {best_num_dense_layers}\")\n",
    "\n",
    "for i in range(best_num_dense_layers):\n",
    "    print(f\"\\n  --- Dense Layer {i+1} ---\")\n",
    "    best_dense_units = hp_values.get(f'dense_units_{i}')\n",
    "    best_dense_activation = hp_values.get(f'dense_activation_{i}')\n",
    "    print(f\"    Units: {best_dense_units}\")\n",
    "    print(f\"    Activation: {best_dense_activation}\")\n",
    "\n",
    "    # Check L1 regularization\n",
    "    use_l1_reg_dense_key = f'use_l1_reg_dense_{i}'\n",
    "    if hp_values.get(use_l1_reg_dense_key, False):\n",
    "        best_l1_reg_dense = hp_values.get(f'l1_reg_dense_{i}')\n",
    "        print(f\"    L1 Regularization: {best_l1_reg_dense:.6f}\")\n",
    "    else:\n",
    "        print(f\"    L1 Regularization: Not used\")\n",
    "\n",
    "    # Check L2 regularization\n",
    "    use_l2_reg_dense_key = f'use_l2_reg_dense_{i}'\n",
    "    if hp_values.get(use_l2_reg_dense_key, False):\n",
    "        best_l2_reg_dense = hp_values.get(f'l2_reg_dense_{i}')\n",
    "        print(f\"    L2 Regularization: {best_l2_reg_dense:.6f}\")\n",
    "    else:\n",
    "        print(f\"    L2 Regularization: Not used\")\n",
    "\n",
    "    # Check if dropout after Dense was enabled for this layer\n",
    "    dropout_dense_key = f'dropout_dense_{i}'\n",
    "    if hp_values.get(dropout_dense_key, False):\n",
    "        best_dropout_rate_dense = hp_values.get(f'dropout_rate_dense_{i}')\n",
    "        print(f\"    Dropout Rate (after Dense): {best_dropout_rate_dense:.2f}\")\n",
    "    else:\n",
    "        print(f\"    Dropout (after Dense): Not used\")\n",
    "\n",
    "print(\"\\n----------------------------------\")\n",
    "\n",
    "# --- Get the best model(s) ---\n",
    "# This part remains the same as it correctly interacts with the tuner object\n",
    "best_models = tuner.get_best_models(num_models=1)\n",
    "best_model = best_models[0]\n",
    "\n",
    "print(\"\\nSummary of the best model:\")\n",
    "best_model.summary()\n",
    "\n",
    "# --- Print a summary of the tuning results ---\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20126242",
   "metadata": {
    "id": "20126242"
   },
   "source": [
    "### 3.2 Training the Final Model with Optimal Hyperparameters\n",
    "\n",
    "A new model (`final_model`) is instantiated using the `best_hps` found by the tuner. This model now has the optimized architecture and configuration. This model is then trained on the *full* `X_train` and `y_train` datasets. This is a common practice to leverage all available training data once the optimal hyperparameters are identified, as the validation set was only used for hyperparameter selection and early stopping.\n",
    "\n",
    "We use `EarlyStopping` and `ModelCheckpoint` callbacks to ensure the model training is robust and the best weights are saved.\n",
    "* **`EarlyStopping(patience=10, monitor='loss')`**: Stops training if the training loss doesn't improve for 10 consecutive epochs. (Note: For final training on the full set, sometimes monitoring training loss directly or a larger patience is chosen, as there's no separate validation set to monitor here).\n",
    "* **`ModelCheckpoint(filepath='model/final_lstm_model.keras', save_best_only=True)`**: Saves the model with the best performance (lowest training loss in this case) to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1576086",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1751872127390,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "c1576086"
   },
   "outputs": [],
   "source": [
    "# --- Train the best model with optimal hyperparameters ---\n",
    "print(\"\\nTraining the best model with optimal hyperparameters...\")\n",
    "\n",
    "# Re-build the model using the best hyperparameters\n",
    "final_model = build_lstm_model_(best_hps)\n",
    "\n",
    "# Train the final model (you might use more epochs here)\n",
    "history = final_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=100, # Train for more epochs\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[stop_early], # Use early stopping again\n",
    "    batch_size=2048,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nFinal model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bca75",
   "metadata": {
    "id": "137bca75"
   },
   "source": [
    "### 3.3 Saving the Final Model\n",
    "\n",
    "After training the final model, it's saved in the `.keras` format. This allows for easy loading and deployment of the trained model without needing to redefine its architecture or weights.\n",
    "\n",
    "The output confirms the path where the model has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f2074",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1751872127391,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "e57f2074"
   },
   "outputs": [],
   "source": [
    "# Save the model in .keras format\n",
    "model_save_path_keras = \"model/final_lstm_model.keras\"\n",
    "final_model.save(model_save_path_keras)\n",
    "print(f\"Final model saved in .keras format to: {model_save_path_keras}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290f672",
   "metadata": {
    "id": "4290f672"
   },
   "source": [
    "### 3.5 Making Predictions with the Final Model\n",
    "\n",
    "The loaded `LTSM_model` (which is our `final_model`) is used to make predictions on the `X_test` dataset. Since the model was trained on scaled RUL values, the predictions (`y_pred`) are also scaled. We then use the `rul_scaler` to `inverse_transform` these predictions back to their original RUL scale (number of cycles).\n",
    "\n",
    "The output displays the first 10 actual RUL values from `y_true` and the corresponding predicted RUL values from `predictions`. This allows for a quick visual comparison of how well the model is performing on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d24a13",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1751872127392,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "16d24a13"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_save_path_keras = \"model/final_lstm_model.keras\"\n",
    "LTSM_model = tf.keras.models.load_model(model_save_path_keras)\n",
    "print(f\"\\nModel loaded from .keras format successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb4798",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1751872127392,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "f6bb4798"
   },
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "predictions = rul_scaler.inverse_transform(y_pred)\n",
    "print(f'The first 10 actual RUL are {y_true[:10]}')\n",
    "print(f'The first 10 predicted RUL are {y_pred[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f8abb",
   "metadata": {
    "id": "e80f8abb"
   },
   "source": [
    "### 3.6 Quantitative Evaluation of the Final Model\n",
    "\n",
    "The `evaluate_predictions` function (presumably from `custom_functions.py`) is called to calculate key regression metrics, providing a quantitative assessment of the model's accuracy on the test set.\n",
    "\n",
    "* **Mean Absolute Error (MAE)**: Measures the average magnitude of the errors. Lower MAE indicates better performance.\n",
    "* **Root Mean Squared Error (RMSE)**: A quadratic scoring rule that measures the average magnitude of the error. It gives a relatively high weight to large errors.\n",
    "* **RUL Score**: A custom metric (often used in the C-MAPSS challenge) that penalizes late predictions more heavily than early predictions, reflecting the criticality of proactive maintenance.\n",
    "\n",
    "*You should update this section with the actual values output by `evaluate_predictions` to interpret the model's performance on the test data based on these metrics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a451a1",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1751872127393,
     "user": {
      "displayName": "Teamer Boii",
      "userId": "14794292507610291870"
     },
     "user_tz": -330
    },
    "id": "98a451a1"
   },
   "outputs": [],
   "source": [
    "evaluate_predictions(y_true,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d475cb",
   "metadata": {
    "id": "c7d475cb"
   },
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "This notebook successfully implemented a comprehensive hyperparameter tuning process for an LSTM-based RUL prediction model using Keras Tuner. The optimal hyperparameters were identified, and a final model was trained and evaluated on an unseen test dataset.\n",
    "\n",
    "**Summary of Findings (to be filled based on your results):**\n",
    "* The hyperparameter tuning process identified a model configuration that achieved a **[Insert best val_loss and val_mae from tuner here]** validation loss/MAE.\n",
    "* The final model, trained with the optimized hyperparameters, yielded **[Insert MAE, RMSE, RUL Score from final evaluation here]** on the test set.\n",
    "* The True vs. Predicted RUL plot visually confirms the model's ability to predict RUL, though there may be certain ranges where predictions are more scattered (e.g., very high RUL values).\n",
    "\n",
    "**Potential Future Work:**\n",
    "* **More Extensive Tuning**: Explore a broader range of hyperparameters or more advanced tuning algorithms (e.g., Bayesian Optimization).\n",
    "* **Error Analysis**: Conduct a deeper analysis of prediction errors, especially for outliers, to understand their root causes (e.g., specific engine profiles, operational settings).\n",
    "* **Uncertainty Estimation**: Implement techniques to quantify the uncertainty of RUL predictions, providing confidence intervals for maintenance decisions.\n",
    "* **Deployment Considerations**: Discuss how this model could be deployed in a real-world scenario (e.g., using TensorFlow Serving).\n",
    "\n",
    "This notebook demonstrates proficiency in advanced deep learning model development and optimization, making it a strong asset for your portfolio."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rapids_python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
