{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af82fc44",
   "metadata": {},
   "source": [
    "# Predictive Maintenance: Remaining Useful Life (RUL) Prediction for Turbofan Engines\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates a predictive maintenance solution for turbofan engines. The goal is to predict the Remaining Useful Life (RUL) of aircraft engines based on sensor data, allowing for proactive maintenance scheduling and preventing unexpected failures. This project leverages time-series data and Long Short-Term Memory (LSTM) neural networks, a type of recurrent neural network (RNN) particularly well-suited for sequence prediction problems.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used in this project is the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset, specifically FD001. It comprises multivariate time series data from multiple turbofan engines. Each engine starts with different initial wear and manufacturing variations, and operates under different operational settings. The data records various sensor measurements at each operational cycle until failure.\n",
    "\n",
    "**Key features of the dataset:**\n",
    "\n",
    "* `engine_id`: Unique identifier for each engine.\n",
    "* `cycle`: Operational cycle number.\n",
    "* `op_setting_1`, `op_setting_2`, `op_setting_3`: Operational settings.\n",
    "* `sensor_1` to `sensor_21`: Various sensor measurements.\n",
    "* `sensor_22`, `sensor_23`: Redundant or constant sensor readings (to be dropped)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49819046",
   "metadata": {},
   "source": [
    "# Predictive Maintenance: Remaining Useful Life (RUL) Prediction for Turbofan Engines\n",
    "\n",
    "*Author: Krishna Singh*\n",
    "*Date: July 5, 2025*\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Project Overview](#Project-Overview)\n",
    "2.  [Dataset](#Dataset)\n",
    "3.  [Data Loading and Initial Exploration](#1.-Data-Loading-and-Initial-Exploration)\n",
    "    * [1.1 Loading Raw Data Files](#1.1-Loading-Raw-Data-Files)\n",
    "    * [1.2 Initial Data Inspection](#1.2-Initial-Data-Inspection)\n",
    "    * [1.3 Verifying Unique Engine IDs in Each Dataset](#1.3-Verifying-Unique-Engine-IDs-in-Each-Dataset)\n",
    "    * [1.4 Consolidating Training Data](#1.4-Consolidating-Training-Data)\n",
    "    * [1.5 Identifying Low-Variance Features](#1.5-Identifying-Low-Variance-Features)\n",
    "    * [1.6 Data Types and Missing Values](#1.6-Data-Types-and-Missing-Values)\n",
    "    * [1.7 Statistical Summary of Features](#1.7-Statistical-Summary-of-Features)\n",
    "4.  [Feature Engineering: Remaining Useful Life (RUL) Calculation](#2.-Feature-Engineering:-Remaining-Useful-Life-(RUL)-Calculation)\n",
    "    * [2.1 Verifying RUL at End-of-Life](#2.1-Verifying-RUL-at-End-of-Life)\n",
    "5.  [Data Preprocessing: Scaling and Sequence Generation](#3.-Data-Preprocessing:-Scaling-and-Sequence-Generation)\n",
    "    * [3.1 Feature Scaling](#3.1-Feature-Scaling)\n",
    "    * [3.2 Saving Scalers](#3.2-Saving-Scalers)\n",
    "6.  [Feature Engineering: Creating Rolling Window Statistics](#4.-Feature-Engineering:-Creating-Rolling-Window-Statistics)\n",
    "    * [4.1 Updated Feature Set for Model Training](#4.1-Updated-Feature-Set-for-Model-Training)\n",
    "7.  [Sequence Generation for LSTM Model](#5.-Sequence-Generation-for-LSTM-Model)\n",
    "    * [5.1 Preparing Test Data Sequences](#5.1-Preparing-Test-Data-Sequences)\n",
    "    * [5.2 Final Data Shapes for LSTM Input](#5.2-Final-Data-Shapes-for-LSTM-Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa222d4b",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "This section focuses on loading the raw sensor data, assigning meaningful column names, and performing initial data inspections to understand its structure and identify any immediate issues like missing values or constant columns.\n",
    "\n",
    "### 1.1 Importing Libraries\n",
    "\n",
    "This cell imports the necessary Python libraries for data manipulation, numerical operations, machine learning preprocessing, and model building:\n",
    "- `pandas` for DataFrame operations.\n",
    "- `numpy` for numerical operations.\n",
    "- `tensorflow` for building and training the neural network model.\n",
    "- `joblib` for saving and loading Python objects, such as scalers.\n",
    "- `sklearn.preprocessing.MinMaxScaler` for data scaling.\n",
    "- `custom_functions` for specific utility functions (e.g., `import_data`, `max_cycles`, `rolling_mean_std`, `create_sequences`, `create_single_last_sequence`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21648154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 09:50:46.936016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751795447.227929    4639 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751795447.302729    4639 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751795447.928373    4639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751795447.928406    4639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751795447.928408    4639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751795447.928410    4639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-06 09:50:47.988515: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from custom_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a79c9",
   "metadata": {},
   "source": [
    "### 1.2 Loading Raw Data Files\n",
    "\n",
    "The C-MAPSS dataset is provided in separate text files for training (FD001-FD004) and testing. We use a custom `import_data` function (defined in `custom_functions.py`) to load these files into pandas DataFrames and assign standard column names as per the dataset documentation. We then display the head of `data_1` to inspect its initial structure.\n",
    "\n",
    "The `head()` below above confirms the data has been loaded correctly with appropriate column names. We can see the `engine_id`, `cycle`, operational settings (`op_setting_1`, `op_setting_2`, `op_setting_3`), and various sensor readings (`sensor_1` to `sensor_21`). This initial view gives us an understanding of the data's raw format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2adb0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   engine_id  cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "0          1      1       34.9983        0.8400         100.0    449.44   \n",
      "1          1      2       41.9982        0.8408         100.0    445.00   \n",
      "2          1      3       24.9988        0.6218          60.0    462.54   \n",
      "3          1      4       42.0077        0.8416         100.0    445.00   \n",
      "4          1      5       25.0005        0.6203          60.0    462.54   \n",
      "\n",
      "   sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_11  sensor_12  \\\n",
      "0    555.32   1358.61   1137.23      5.48  ...      42.02     183.06   \n",
      "1    549.90   1353.22   1125.78      3.91  ...      42.20     130.42   \n",
      "2    537.31   1256.76   1047.45      7.05  ...      36.69     164.22   \n",
      "3    549.51   1354.03   1126.38      3.91  ...      41.96     130.72   \n",
      "4    537.07   1257.71   1047.93      7.05  ...      36.89     164.31   \n",
      "\n",
      "   sensor_13  sensor_14  sensor_15  sensor_17  sensor_18  sensor_19  \\\n",
      "0    2387.72    8048.56     9.3461        334       2223     100.00   \n",
      "1    2387.66    8072.30     9.3774        330       2212     100.00   \n",
      "2    2028.03    7864.87    10.8941        309       1915      84.93   \n",
      "3    2387.61    8068.66     9.3528        329       2212     100.00   \n",
      "4    2028.00    7861.23    10.8963        309       1915      84.93   \n",
      "\n",
      "   sensor_20  sensor_21  \n",
      "0      14.73     8.8071  \n",
      "1      10.41     6.2665  \n",
      "2      14.08     8.6723  \n",
      "3      10.59     6.4701  \n",
      "4      14.13     8.5286  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "data_2 = import_data('train/train_FD002.txt')\n",
    "data_4 = import_data('train/train_FD004.txt')\n",
    "test_df = import_data('test/test_FD001.txt')\n",
    "\n",
    "print(data_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcefa13",
   "metadata": {},
   "source": [
    "### 1.3 Verifying Unique Engine IDs in Each Dataset\n",
    "\n",
    "Before concatenating the datasets, it's important to verify the number of unique engines in each FD (Flight Dataset) file. The output confirms:\n",
    "* **FD002:** 260 unique engines\n",
    "* **FD004:** 249 unique engines\n",
    "\n",
    "This step ensures we understand the scope of each individual training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d569925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "print(data_2['engine_id'].nunique())\n",
    "print(data_4['engine_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8f605",
   "metadata": {},
   "source": [
    "### 1.4 Consolidating Training Data\n",
    "\n",
    "To create a unified training dataset, we first re-index the `engine_id` column for `data_2`, and `data_4`. This ensures that each engine across all four FD datasets has a unique identifier, preventing ID clashes when concatenating. Following re-indexing, all four training DataFrames are concatenated into a single `data` DataFrame, which will be used for training the RUL prediction model. This unified dataset provides a richer and more diverse set of engine degradation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1108088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4['engine_id'] = data_4['engine_id'].replace([i for i in range(1, 250)],[i for i in range(261, 261+249)])\n",
    "\n",
    "data = pd.concat([data_2,data_4],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecd002",
   "metadata": {},
   "source": [
    "### 1.5 Identifying Low-Variance Features\n",
    "\n",
    "We calculate the variance for all columns in the combined training dataset to identify features with very little or no variation. Features with extremely low variance often provide little to no useful information for a predictive model and can sometimes be considered constant or near-constant.\n",
    "\n",
    "The output shows the top 5 least variance columns:\n",
    "* `op_setting_2`: Very low variance, suggesting it might be constant or have minimal changes.\n",
    "* `sensor_15`, `sensor_11`, `sensor_5`, `sensor_19`: Also show relatively low variance compared to other sensors.\n",
    "\n",
    "While some of these (`op_setting_3`, `sensor_1`, `sensor_5`, `sensor_18`, `sensor_19`) are known to be constant or near-constant in the C-MAPSS dataset and are often dropped in literature, we will proceed with the current set for now, relying on the model to learn their significance (or lack thereof). However, for a production system, these might be candidates for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2c8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 least variance columns:\n",
      "op_setting_2     0.096336\n",
      "sensor_15        0.562811\n",
      "sensor_11       10.489534\n",
      "sensor_5        13.094532\n",
      "sensor_19       28.803589\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "variances = data.var()\n",
    "least_variance_columns = variances.sort_values().head(5)\n",
    "print(\"\\nTop 5 least variance columns:\")\n",
    "print(least_variance_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668ce9c",
   "metadata": {},
   "source": [
    "### 1.6 Data Types and Missing Values\n",
    "\n",
    "The `data.info()` output provides a summary of the DataFrame, including the number of non-null entries for each column and their data types.\n",
    "\n",
    "**Observations:**\n",
    "* **No Missing Values:** All columns show `160359 non-null` entries, confirming there are no missing values in the combined training dataset, which simplifies preprocessing.\n",
    "* **Data Types:** Most sensor readings and operational settings are `float64`, while `engine_id`, `cycle`, `sensor_17`, and `sensor_18` are `int64`. These data types are appropriate for numerical analysis.\n",
    "* **Memory Usage:** The DataFrame occupies approximately 29.4 MB of memory.\n",
    "\n",
    "This inspection confirms the data's integrity and readiness for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9619145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 115008 entries, 0 to 115007\n",
      "Data columns (total 24 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   engine_id     115008 non-null  int64  \n",
      " 1   cycle         115008 non-null  int64  \n",
      " 2   op_setting_1  115008 non-null  float64\n",
      " 3   op_setting_2  115008 non-null  float64\n",
      " 4   op_setting_3  115008 non-null  float64\n",
      " 5   sensor_1      115008 non-null  float64\n",
      " 6   sensor_2      115008 non-null  float64\n",
      " 7   sensor_3      115008 non-null  float64\n",
      " 8   sensor_4      115008 non-null  float64\n",
      " 9   sensor_5      115008 non-null  float64\n",
      " 10  sensor_6      115008 non-null  float64\n",
      " 11  sensor_7      115008 non-null  float64\n",
      " 12  sensor_8      115008 non-null  float64\n",
      " 13  sensor_9      115008 non-null  float64\n",
      " 14  sensor_11     115008 non-null  float64\n",
      " 15  sensor_12     115008 non-null  float64\n",
      " 16  sensor_13     115008 non-null  float64\n",
      " 17  sensor_14     115008 non-null  float64\n",
      " 18  sensor_15     115008 non-null  float64\n",
      " 19  sensor_17     115008 non-null  int64  \n",
      " 20  sensor_18     115008 non-null  int64  \n",
      " 21  sensor_19     115008 non-null  float64\n",
      " 22  sensor_20     115008 non-null  float64\n",
      " 23  sensor_21     115008 non-null  float64\n",
      "dtypes: float64(20), int64(4)\n",
      "memory usage: 21.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a86078",
   "metadata": {},
   "source": [
    "### 1.6 Data Types and Missing Values\n",
    "\n",
    "The `data.info()` output provides a summary of the DataFrame, including the number of non-null entries for each column and their data types.\n",
    "\n",
    "**Observations:**\n",
    "* **No Missing Values:** All columns show `160359 non-null` entries, confirming there are no missing values in the combined training dataset, which simplifies preprocessing.\n",
    "* **Data Types:** Most sensor readings and operational settings are `float64`, while `engine_id`, `cycle`, `sensor_17`, and `sensor_18` are `int64`. These data types are appropriate for numerical analysis.\n",
    "* **Memory Usage:** The DataFrame occupies approximately 29.4 MB of memory.\n",
    "\n",
    "This inspection confirms the data's integrity and readiness for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f7eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           engine_id          cycle   op_setting_1   op_setting_2  \\\n",
      "count  115008.000000  115008.000000  115008.000000  115008.000000   \n",
      "mean      265.950395     122.552257      23.999161       0.571679   \n",
      "std       146.004517      81.777999      14.765080       0.310381   \n",
      "min         1.000000       1.000000       0.000000       0.000000   \n",
      "25%       140.000000      57.000000      10.004600       0.250700   \n",
      "50%       274.000000     113.000000      25.001400       0.700000   \n",
      "75%       393.000000     173.000000      41.998000       0.840000   \n",
      "max       509.000000     543.000000      42.008000       0.842000   \n",
      "\n",
      "        op_setting_3       sensor_1       sensor_2       sensor_3  \\\n",
      "count  115008.000000  115008.000000  115008.000000  115008.000000   \n",
      "mean       94.038328     472.895417     579.538010    1418.866258   \n",
      "std        14.245249      26.414703      37.317816     106.068820   \n",
      "min        60.000000     445.000000     535.480000    1242.670000   \n",
      "25%       100.000000     445.000000     549.440000    1351.600000   \n",
      "50%       100.000000     462.540000     555.870000    1368.450000   \n",
      "75%       100.000000     491.190000     607.200000    1498.420000   \n",
      "max       100.000000     518.670000     644.520000    1613.000000   \n",
      "\n",
      "            sensor_4       sensor_5  ...      sensor_11      sensor_12  \\\n",
      "count  115008.000000  115008.000000  ...  115008.000000  115008.000000   \n",
      "mean     1203.563853       8.031794  ...      42.926248     266.424057   \n",
      "std       119.244666       3.618637  ...       3.238755     138.096403   \n",
      "min      1023.770000       3.910000  ...      36.040000     128.310000   \n",
      "25%      1121.520000       3.910000  ...      41.830000     134.470000   \n",
      "50%      1137.980000       7.050000  ...      42.370000     183.290000   \n",
      "75%      1304.740000      10.520000  ...      45.290000     371.320000   \n",
      "max      1440.770000      14.620000  ...      48.510000     537.490000   \n",
      "\n",
      "           sensor_13      sensor_14      sensor_15      sensor_17  \\\n",
      "count  115008.000000  115008.000000  115008.000000  115008.000000   \n",
      "mean     2334.488199    8067.244283       9.306194     348.016877   \n",
      "std       128.136761      85.284151       0.750207      27.784395   \n",
      "min      2027.570000    7845.780000       8.175700     302.000000   \n",
      "25%      2387.910000    8062.410000       8.665200     330.000000   \n",
      "50%      2388.070000    8083.210000       9.283000     335.000000   \n",
      "75%      2388.170000    8127.780000       9.377500     369.000000   \n",
      "max      2390.490000    8268.500000      11.066900     399.000000   \n",
      "\n",
      "           sensor_18      sensor_19      sensor_20      sensor_21  \n",
      "count  115008.000000  115008.000000  115008.000000  115008.000000  \n",
      "mean     2228.703534      97.753940      20.829258      12.497693  \n",
      "std       145.404359       5.366898       9.905132       5.943547  \n",
      "min      1915.000000      84.930000      10.160000       6.010500  \n",
      "25%      2212.000000     100.000000      10.930000       6.561650  \n",
      "50%      2223.000000     100.000000      14.910000       8.945200  \n",
      "75%      2324.000000     100.000000      28.510000      17.107000  \n",
      "max      2388.000000     100.000000      39.890000      23.885200  \n",
      "\n",
      "[8 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e7012",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering: Remaining Useful Life (RUL) Calculation\n",
    "\n",
    "Predicting RUL requires a target variable that represents the time remaining until engine failure. For the C-MAPSS dataset, this is not directly provided but can be derived. The `max_cycles` function (from `custom_functions.py`) calculates the maximum `cycle` for each `engine_id` and then computes the RUL as `max_cycle_for_engine - current_cycle`. This creates a linearly decreasing target variable for each engine, going from its maximum operational cycle down to 0 at failure.\n",
    "\n",
    "The `head()` output below for `engine_id`, `cycle`, and `RUL` demonstrates this calculation for the first engine. As the `cycle` increases, the `RUL` value correctly decreases, starting from 191 cycles remaining down to 0 (which will be seen at the tail of the data for each engine). This is a critical step in preparing the target variable for our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bbdbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   engine_id  cycle  RUL\n",
      "0          1      1  148\n",
      "1          1      2  147\n",
      "2          1      3  146\n",
      "3          1      4  145\n",
      "4          1      5  144\n"
     ]
    }
   ],
   "source": [
    "data = max_cycles(data)\n",
    "\n",
    "\n",
    "# Inspecting the RUL column\n",
    "print(data[['engine_id', 'cycle', 'RUL']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91754fc2",
   "metadata": {},
   "source": [
    "### 2.1 Verifying RUL at End-of-Life\n",
    "\n",
    "By inspecting the `tail()` of the combined dataset, we can observe the behavior of the `RUL` column, particularly for engines nearing their failure point. The output above shows the last few cycles of `engine_id` 609. As expected, the `RUL` value decreases to `0` at the final recorded cycle (`cycle` 255 for `engine_id` 609), confirming the correct calculation of the Remaining Useful Life. This validates that our target variable is appropriately defined for the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b826c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        engine_id  cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "115003        509    251        9.9998        0.2500         100.0    489.05   \n",
      "115004        509    252        0.0028        0.0015         100.0    518.67   \n",
      "115005        509    253        0.0029        0.0000         100.0    518.67   \n",
      "115006        509    254       35.0046        0.8400         100.0    449.44   \n",
      "115007        509    255       42.0030        0.8400         100.0    445.00   \n",
      "\n",
      "        sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_12  sensor_13  \\\n",
      "115003    605.33   1516.36   1315.28     10.52  ...     380.16    2388.73   \n",
      "115004    643.42   1598.92   1426.77     14.62  ...     535.02    2388.46   \n",
      "115005    643.68   1607.72   1430.56     14.62  ...     535.41    2388.48   \n",
      "115006    555.77   1381.29   1148.18      5.48  ...     187.92    2388.83   \n",
      "115007    549.85   1369.75   1147.45      3.91  ...     134.32    2388.66   \n",
      "\n",
      "        sensor_14  sensor_15  sensor_17  sensor_18  sensor_19  sensor_20  \\\n",
      "115003    8185.69     8.4541        372       2319      100.0      29.11   \n",
      "115004    8185.47     8.2221        396       2388      100.0      39.38   \n",
      "115005    8193.94     8.2525        395       2388      100.0      39.78   \n",
      "115006    8125.64     9.0515        337       2223      100.0      15.26   \n",
      "115007    8144.33     9.1207        333       2212      100.0      10.66   \n",
      "\n",
      "        sensor_21  RUL  \n",
      "115003    17.5234    4  \n",
      "115004    23.7151    3  \n",
      "115005    23.8270    2  \n",
      "115006     9.0774    1  \n",
      "115007     6.4341    0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Show df.tail() for an engine to verify RUL decreases to 0\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac0736",
   "metadata": {},
   "source": [
    "### 2.2 Final Columns After RUL Addition\n",
    "\n",
    "This output shows all the columns currently present in our consolidated training DataFrame, including the newly added `RUL` column. This confirms that all relevant features and the target variable are ready for the next stages of preprocessing, such as feature scaling and sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9366a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['engine_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3',\n",
      "       'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6',\n",
      "       'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12',\n",
      "       'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_18',\n",
      "       'sensor_19', 'sensor_20', 'sensor_21', 'RUL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the columns of dataset\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6d0f1",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing: Scaling and Sequence Generation\n",
    "\n",
    "Neural networks, especially LSTMs, perform best when input features are scaled to a common range. This prevents features with larger numerical values from dominating the learning process.\n",
    "\n",
    "### 3.1 Feature Scaling\n",
    "\n",
    "We use `MinMaxScaler` from `sklearn.preprocessing` to scale all operational settings and sensor measurements (`feature_cols`) to a range between 0 and 1. The `RUL` target variable is also scaled using a separate `MinMaxScaler` (`rul_scaler`). This is important because RUL values can be large, and scaling them helps the model converge faster and more stably.\n",
    "\n",
    "* **`feature_scaler.pkl`**: The scaler fitted on `feature_cols` from the training data is saved to ensure that the same scaling transformation can be applied to new, unseen test data.\n",
    "* **`rul_scaler.pkl`**: Similarly, the scaler for the `RUL` target variable is saved. This will be crucial for inverse-transforming the model's predicted RUL values back to their original scale for meaningful interpretation.\n",
    "\n",
    "The `head()` output above shows the training DataFrame after all features and the RUL target have been scaled. Notice how all values are now within the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d03f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Dataframe after Scaling:\n",
      "   engine_id     cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "0          1  0.000000      0.833134      0.997625           1.0  0.060269   \n",
      "1          1  0.001845      0.999767      0.998575           1.0  0.000000   \n",
      "2          1  0.003690      0.595096      0.738480           0.0  0.238089   \n",
      "3          1  0.005535      0.999993      0.999525           1.0  0.000000   \n",
      "4          1  0.007380      0.595137      0.736698           0.0  0.238089   \n",
      "\n",
      "   sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_12  sensor_13  \\\n",
      "0  0.181952  0.313072  0.272086  0.146592  ...   0.133804   0.992367   \n",
      "1  0.132245  0.298518  0.244628  0.000000  ...   0.005157   0.992202   \n",
      "2  0.016783  0.038047  0.056787  0.293184  ...   0.087761   0.001267   \n",
      "3  0.128668  0.300705  0.246067  0.000000  ...   0.005890   0.992064   \n",
      "4  0.014582  0.040612  0.057938  0.293184  ...   0.087981   0.001185   \n",
      "\n",
      "   sensor_14  sensor_15  sensor_17  sensor_18  sensor_19  sensor_20  \\\n",
      "0   0.479703   0.404815   0.329897   0.651163        1.0   0.153717   \n",
      "1   0.535863   0.415641   0.288660   0.627907        1.0   0.008409   \n",
      "2   0.045160   0.940232   0.072165   0.000000        0.0   0.131853   \n",
      "3   0.527252   0.407132   0.278351   0.627907        1.0   0.014464   \n",
      "4   0.036549   0.940993   0.072165   0.000000        0.0   0.133535   \n",
      "\n",
      "   sensor_21       RUL  \n",
      "0   0.156456  0.273063  \n",
      "1   0.014322  0.271218  \n",
      "2   0.148914  0.269373  \n",
      "3   0.025712  0.267528  \n",
      "4   0.140875  0.265683  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Defining feature columns\n",
    "feature_cols = [col for col in data.columns if col not in ['RUL',\"engine_id\"]]\n",
    "print(feature_cols)\n",
    "\n",
    "# Scaling the features that will be used to train and test\n",
    "scaler = MinMaxScaler()\n",
    "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
    "test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "# Scaling the labels that will be used to train\n",
    "rul_scaler = MinMaxScaler()\n",
    "data['RUL'] = rul_scaler.fit_transform(data['RUL'].values.reshape(-1, 1))\n",
    "\n",
    "# Checking the dataframe after scaling\n",
    "print('The Training Dataframe after Scaling:')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162cdb5",
   "metadata": {},
   "source": [
    "### 3.2 Saving Scalers\n",
    "\n",
    "It is crucial to save the `MinMaxScaler` objects (`feature_scaler.pkl` and `rul_scaler.pkl`) after fitting them to the training data. This ensures that:\n",
    "1.  **Consistency:** The exact same scaling transformation (based on the training data's min/max values) can be applied to the test data or any future unseen data.\n",
    "2.  **Inverse Transformation:** The `rul_scaler` can be used to convert the model's predicted, scaled RUL values back into their original, interpretable cycle counts.\n",
    "\n",
    "This practice prevents data leakage from the test set and allows for consistent deployment of the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "414b40cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rul_scaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's good practice to save your scalers for later use on test data and for inverse transformation.\n",
    "# Example: saving the scalers (though not runnable without actual saving mechanism)\n",
    "import joblib\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(rul_scaler, 'rul_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c09a5",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Creating Rolling Window Statistics\n",
    "\n",
    "To capture the temporal dependencies and trends in the sensor data, we generate **rolling mean** and **rolling standard deviation** features for the selected sensor measurements. A `window_size` of 30 cycles is used. This means for each cycle, the rolling features are calculated based on the preceding 30 cycles (including the current one).\n",
    "\n",
    "* **Rolling Mean:** Provides a smoothed trend of the sensor readings, indicating general deterioration.\n",
    "* **Rolling Standard Deviation:** Captures the variability or volatility of sensor readings within the window, which can be an indicator of increasing instability as an engine degrades.\n",
    "\n",
    "The output above demonstrates the `sensor_2` original values alongside its 30-cycle rolling mean and standard deviation for the first engine. Notice how the rolling mean starts from the current value and gradually smooths out as more data points fill the window. The rolling standard deviation provides insight into the local variability of the sensor. These features are vital for LSTMs to learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c7b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Displaying Rolling Features for Engine 1 (first 30 cycles) ---\n",
      "    engine_id     cycle  sensor_2  sensor_2_rolling_mean_30  \\\n",
      "0           1  0.000000  0.181952                  0.181952   \n",
      "1           1  0.001845  0.132245                  0.157098   \n",
      "2           1  0.003690  0.016783                  0.110326   \n",
      "3           1  0.005535  0.128668                  0.114912   \n",
      "4           1  0.007380  0.014582                  0.094846   \n",
      "5           1  0.009225  0.014123                  0.081392   \n",
      "6           1  0.011070  0.130778                  0.088447   \n",
      "7           1  0.012915  0.659941                  0.159884   \n",
      "8           1  0.014760  0.127018                  0.156232   \n",
      "9           1  0.016605  0.127018                  0.153311   \n",
      "10          1  0.018450  0.131420                  0.151321   \n",
      "11          1  0.020295  0.983309                  0.220653   \n",
      "12          1  0.022140  0.662051                  0.254607   \n",
      "13          1  0.023985  0.128302                  0.245585   \n",
      "14          1  0.025830  0.633621                  0.271454   \n",
      "\n",
      "    sensor_2_rolling_std_30  \n",
      "0                  0.035148  \n",
      "1                  0.035148  \n",
      "2                  0.084738  \n",
      "3                  0.069793  \n",
      "4                  0.075277  \n",
      "5                  0.074962  \n",
      "6                  0.070931  \n",
      "7                  0.212457  \n",
      "8                  0.199037  \n",
      "9                  0.187881  \n",
      "10                 0.178362  \n",
      "11                 0.294287  \n",
      "12                 0.307205  \n",
      "13                 0.297077  \n",
      "14                 0.303297  \n"
     ]
    }
   ],
   "source": [
    "# Defining the necessary variables for Forging Insights\n",
    "window_size = 30\n",
    "selected_sensors = [col for col in feature_cols if col not in ['cycle','op_setting_1','op_setting_2','op_setting_3']]\n",
    "\n",
    "data = rolling_mean_std(data,window_size,feature_cols)\n",
    "test = rolling_mean_std(test_df,window_size,feature_cols)\n",
    "\n",
    "print(\"\\n--- Displaying Rolling Features for Engine 1 (first 30 cycles) ---\")\n",
    "# Show how the rolling features look for engine 1\n",
    "engine1_df = data[data['engine_id'] == 1]\n",
    "print(engine1_df[['engine_id', 'cycle', 'sensor_2', f'sensor_2_rolling_mean_{window_size}', f'sensor_2_rolling_std_{window_size}']].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab9768",
   "metadata": {},
   "source": [
    "### 4.1 Updated Feature Set for Model Training\n",
    "\n",
    "After generating the rolling mean and standard deviation for all relevant sensor features, our feature set (`feature_cols`) has significantly expanded. The output below confirms the new list of features that will be used as input to the LSTM model. It now includes the original `cycle` and `op_setting` features, plus the original sensor readings, and their corresponding rolling mean and standard deviation features. This comprehensive set aims to provide the model with a rich representation of the engine's health over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c1d5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21', 'sensor_1_rolling_mean_30', 'sensor_1_rolling_std_30', 'sensor_2_rolling_mean_30', 'sensor_2_rolling_std_30', 'sensor_3_rolling_mean_30', 'sensor_3_rolling_std_30', 'sensor_4_rolling_mean_30', 'sensor_4_rolling_std_30', 'sensor_5_rolling_mean_30', 'sensor_5_rolling_std_30', 'sensor_6_rolling_mean_30', 'sensor_6_rolling_std_30', 'sensor_7_rolling_mean_30', 'sensor_7_rolling_std_30', 'sensor_8_rolling_mean_30', 'sensor_8_rolling_std_30', 'sensor_9_rolling_mean_30', 'sensor_9_rolling_std_30', 'sensor_11_rolling_mean_30', 'sensor_11_rolling_std_30', 'sensor_12_rolling_mean_30', 'sensor_12_rolling_std_30', 'sensor_13_rolling_mean_30', 'sensor_13_rolling_std_30', 'sensor_14_rolling_mean_30', 'sensor_14_rolling_std_30', 'sensor_15_rolling_mean_30', 'sensor_15_rolling_std_30', 'sensor_17_rolling_mean_30', 'sensor_17_rolling_std_30', 'sensor_18_rolling_mean_30', 'sensor_18_rolling_std_30', 'sensor_19_rolling_mean_30', 'sensor_19_rolling_std_30', 'sensor_20_rolling_mean_30', 'sensor_20_rolling_std_30', 'sensor_21_rolling_mean_30', 'sensor_21_rolling_std_30']\n",
      "(61)\n"
     ]
    }
   ],
   "source": [
    "# updating the feature_cols\n",
    "feature_cols = [col for col in data.columns if col not in ['engine_id','RUL']]\n",
    "print(feature_cols)\n",
    "print(f'({len(feature_cols)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70c2bd",
   "metadata": {},
   "source": [
    "## 5. Sequence Generation for LSTM Model\n",
    "\n",
    "LSTMs require input data to be in a sequence format (samples, timesteps, features). For our RUL prediction task, each sequence represents a fixed \"look-back window\" of an engine's operational history. The `create_sequences` function (from `custom_functions.py`) is used to transform our flattened DataFrame into this 3D format.\n",
    "\n",
    "For each engine:\n",
    "1.  It iterates through the engine's data, creating sequences of `sequence_length` (e.g., 30) cycles.\n",
    "2.  Each sequence's target label is the RUL value at the *end* of that sequence.\n",
    "\n",
    "This approach allows the LSTM to learn the temporal patterns leading up to a specific RUL value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "711a1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence length (hyperparameter)\n",
    "sequence_length = 30 # Let's start with a look-back window of cycles\n",
    "\n",
    "# Initialize empty lists to store all sequences and labels from all engines\n",
    "X_train_sequences = []\n",
    "y_train_labels = []\n",
    "\n",
    "# Group the DataFrame by engine_id and iterate through each group (each engine)\n",
    "# This is crucial to keep sequences from different engines separate\n",
    "for engine_id, engine_df in data.groupby('engine_id'):\n",
    "    # Generate sequences and labels for the current engine\n",
    "    sequences_X, labels_y = create_sequences(engine_df, sequence_length, feature_cols)\n",
    "    \n",
    "    # Extend the main lists with the sequences and labels from this engine\n",
    "    X_train_sequences.extend(sequences_X)\n",
    "    y_train_labels.extend(labels_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b557b0",
   "metadata": {},
   "source": [
    "### 5.1 Preparing Test Data Sequences\n",
    "\n",
    "For the test set, we are interested in predicting the RUL for each engine based on its *last available operational cycle*. The `create_single_last_sequence` function (from `custom_functions.py`) is designed for this purpose. For each engine in the test dataset, it extracts only the final `sequence_length` (e.g., 30) cycles as a single sequence. This single sequence represents the most recent operational history, which is then fed into the trained LSTM model to predict the RUL. This mimics a real-world scenario where a model would predict RUL based on the current observed state of an engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532de8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the sequences for test prediction\n",
    "X_test_sequences = []\n",
    "\n",
    "# Iterate through each engine in your test_df\n",
    "for engine_id, engine_df_group in test_df.groupby('engine_id'): # Use engine_id as column name\n",
    "    # Ensure you pass the grouped DataFrame (engine_df_group) to the function\n",
    "    sequence = create_single_last_sequence(engine_df_group.copy(), sequence_length, feature_cols)\n",
    "    \n",
    "    if sequence is not None:\n",
    "        X_test_sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c4cf5",
   "metadata": {},
   "source": [
    "### 5.2 Final Data Shapes for LSTM Input\n",
    "\n",
    "After sequence generation, the data is converted into NumPy arrays, which is the required input format for Keras/TensorFlow models.\n",
    "\n",
    "The output confirms the shapes of our processed arrays:\n",
    "* **`X_train` (Training Sequences):** `(142698, 30, 61)`\n",
    "    * **142698:** Number of training samples (sequences).\n",
    "    * **30:** `sequence_length` (timesteps per sequence). This means each input to the LSTM will consist of 30 historical cycles.\n",
    "    * **61:** Number of features per timestep (the `feature_cols`).\n",
    "* **`y_train` (Training Labels):** `(142698, 1)`\n",
    "    * **142698:** Number of corresponding RUL labels for each sequence.\n",
    "    * **1:** Each label is a single RUL value.\n",
    "* **`X_test` (Test Sequences):** Will have a shape like `(num_test_engines, 30, 61)` (though not explicitly printed here, it's inferred).\n",
    "\n",
    "These reshaped arrays are now in the correct format to be fed into an LSTM neural network for training and prediction. The saving of `X_train.npy`, `X_test.npy`, and `y_train.npy` ensures that these preprocessed datasets can be easily loaded for model training without re-running the entire preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e734e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (sequences): (100247, 30, 61)\n",
      "Shape of y_train (RUL labels): (100247, 1)\n",
      "\n",
      "First sequence from X_train:\n",
      "[[0.00000000e+00 8.33134165e-01 9.97624703e-01 ... 1.02748109e-01\n",
      "  1.56455773e-01 1.00503812e-01]\n",
      " [1.84501845e-03 9.99766711e-01 9.98574822e-01 ... 1.02748109e-01\n",
      "  8.53888457e-02 1.00503812e-01]\n",
      " [3.69003690e-03 5.95096172e-01 7.38479810e-01 ... 7.83484323e-02\n",
      "  1.06564026e-01 7.99729512e-02]\n",
      " ...\n",
      " [4.98154982e-02 5.23709770e-05 0.00000000e+00 ... 2.79075281e-01\n",
      "  1.96865442e-01 2.76437467e-01]\n",
      " [5.16605166e-02 1.66634927e-05 0.00000000e+00 ... 3.09139835e-01\n",
      "  2.23182526e-01 3.06224670e-01]\n",
      " [5.35055351e-02 4.76242620e-01 8.32660333e-01 ... 3.07530130e-01\n",
      "  2.31916247e-01 3.04677394e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the lists into NumPy arrays for Keras\n",
    "# This is where we get our 3D array (samples, timesteps, features)\n",
    "X_train = np.array(X_train_sequences)\n",
    "y_train = np.array(y_train_labels)\n",
    "X_test = np.array(X_test_sequences)\n",
    "\n",
    "# Saving The Processed Test set for late use\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "\n",
    "# Reshape y_train to be 2D for model fitting (if not already)\n",
    "# This step is good practice to ensure compatibility with Keras\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "# Checking all the Outputs of create_sequences\n",
    "print(\"Shape of X_train (sequences):\", X_train.shape)\n",
    "print(\"Shape of y_train (RUL labels):\", y_train.shape)\n",
    "print(\"\\nFirst sequence from X_train:\")\n",
    "print(X_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
