{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21648154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from custom_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2adb0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   engine_id  cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "0          1      1       -0.0007       -0.0004         100.0    518.67   \n",
      "1          1      2        0.0019       -0.0003         100.0    518.67   \n",
      "2          1      3       -0.0043        0.0003         100.0    518.67   \n",
      "3          1      4        0.0007        0.0000         100.0    518.67   \n",
      "4          1      5       -0.0019       -0.0002         100.0    518.67   \n",
      "\n",
      "   sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_11  sensor_12  \\\n",
      "0    641.82   1589.70   1400.60     14.62  ...      47.47     521.66   \n",
      "1    642.15   1591.82   1403.14     14.62  ...      47.49     522.28   \n",
      "2    642.35   1587.99   1404.20     14.62  ...      47.27     522.42   \n",
      "3    642.35   1582.79   1401.87     14.62  ...      47.13     522.86   \n",
      "4    642.37   1582.85   1406.22     14.62  ...      47.28     522.19   \n",
      "\n",
      "   sensor_13  sensor_14  sensor_15  sensor_17  sensor_18  sensor_19  \\\n",
      "0    2388.02    8138.62     8.4195        392       2388      100.0   \n",
      "1    2388.07    8131.49     8.4318        392       2388      100.0   \n",
      "2    2388.03    8133.23     8.4178        390       2388      100.0   \n",
      "3    2388.08    8133.83     8.3682        392       2388      100.0   \n",
      "4    2388.04    8133.80     8.4294        393       2388      100.0   \n",
      "\n",
      "   sensor_20  sensor_21  \n",
      "0      39.06    23.4190  \n",
      "1      39.00    23.4236  \n",
      "2      38.95    23.3442  \n",
      "3      38.88    23.3739  \n",
      "4      38.90    23.4044  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "data_1 = import_data('re_train_data/train_FD001.txt')\n",
    "data_3 = import_data('re_train_data/train_FD003.txt')\n",
    "\n",
    "print(data_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d569925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(data_1['engine_id'].nunique())\n",
    "print(data_3['engine_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1108088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3['engine_id'] = data_1['engine_id'].replace([i for i in range(1, 101)],[i for i in range(101, 101+100)])\n",
    "\n",
    "data = pd.concat([data_1,data_3],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668ce9c",
   "metadata": {},
   "source": [
    "### 1.6 Data Types and Missing Values\n",
    "\n",
    "The `data.info()` output provides a summary of the DataFrame, including the number of non-null entries for each column and their data types.\n",
    "\n",
    "**Observations:**\n",
    "* **No Missing Values:** All columns show `160359 non-null` entries, confirming there are no missing values in the combined training dataset, which simplifies preprocessing.\n",
    "* **Data Types:** Most sensor readings and operational settings are `float64`, while `engine_id`, `cycle`, `sensor_17`, and `sensor_18` are `int64`. These data types are appropriate for numerical analysis.\n",
    "* **Memory Usage:** The DataFrame occupies approximately 29.4 MB of memory.\n",
    "\n",
    "This inspection confirms the data's integrity and readiness for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9619145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45351 entries, 0 to 45350\n",
      "Data columns (total 24 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   engine_id     41262 non-null  float64\n",
      " 1   cycle         45351 non-null  int64  \n",
      " 2   op_setting_1  45351 non-null  float64\n",
      " 3   op_setting_2  45351 non-null  float64\n",
      " 4   op_setting_3  45351 non-null  float64\n",
      " 5   sensor_1      45351 non-null  float64\n",
      " 6   sensor_2      45351 non-null  float64\n",
      " 7   sensor_3      45351 non-null  float64\n",
      " 8   sensor_4      45351 non-null  float64\n",
      " 9   sensor_5      45351 non-null  float64\n",
      " 10  sensor_6      45351 non-null  float64\n",
      " 11  sensor_7      45351 non-null  float64\n",
      " 12  sensor_8      45351 non-null  float64\n",
      " 13  sensor_9      45351 non-null  float64\n",
      " 14  sensor_11     45351 non-null  float64\n",
      " 15  sensor_12     45351 non-null  float64\n",
      " 16  sensor_13     45351 non-null  float64\n",
      " 17  sensor_14     45351 non-null  float64\n",
      " 18  sensor_15     45351 non-null  float64\n",
      " 19  sensor_17     45351 non-null  int64  \n",
      " 20  sensor_18     45351 non-null  int64  \n",
      " 21  sensor_19     45351 non-null  float64\n",
      " 22  sensor_20     45351 non-null  float64\n",
      " 23  sensor_21     45351 non-null  float64\n",
      "dtypes: float64(21), int64(3)\n",
      "memory usage: 8.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a86078",
   "metadata": {},
   "source": [
    "### 1.6 Data Types and Missing Values\n",
    "\n",
    "The `data.info()` output provides a summary of the DataFrame, including the number of non-null entries for each column and their data types.\n",
    "\n",
    "**Observations:**\n",
    "* **No Missing Values:** All columns show `160359 non-null` entries, confirming there are no missing values in the combined training dataset, which simplifies preprocessing.\n",
    "* **Data Types:** Most sensor readings and operational settings are `float64`, while `engine_id`, `cycle`, `sensor_17`, and `sensor_18` are `int64`. These data types are appropriate for numerical analysis.\n",
    "* **Memory Usage:** The DataFrame occupies approximately 29.4 MB of memory.\n",
    "\n",
    "This inspection confirms the data's integrity and readiness for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f7eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          engine_id         cycle  op_setting_1  op_setting_2  op_setting_3  \\\n",
      "count  41262.000000  45351.000000  45351.000000  45351.000000       45351.0   \n",
      "mean     101.506568    125.307049     -0.000017      0.000004         100.0   \n",
      "std       57.916271     87.813757      0.002191      0.000294           0.0   \n",
      "min        1.000000      1.000000     -0.008700     -0.000600         100.0   \n",
      "25%       52.000000     57.000000     -0.001500     -0.000200         100.0   \n",
      "50%      100.500000    114.000000      0.000000      0.000000         100.0   \n",
      "75%      152.000000    174.000000      0.001500      0.000300         100.0   \n",
      "max      200.000000    525.000000      0.008700      0.000700         100.0   \n",
      "\n",
      "       sensor_1      sensor_2      sensor_3      sensor_4      sensor_5  ...  \\\n",
      "count  45351.00  45351.000000  45351.000000  45351.000000  4.535100e+04  ...   \n",
      "mean     518.67    642.559339   1589.190970   1406.501317  1.462000e+01  ...   \n",
      "std        0.00      0.524596      6.622906      9.687784  3.552753e-15  ...   \n",
      "min      518.67    640.840000   1564.300000   1377.060000  1.462000e+01  ...   \n",
      "25%      518.67    642.180000   1584.570000   1399.250000  1.462000e+01  ...   \n",
      "50%      518.67    642.520000   1588.800000   1405.510000  1.462000e+01  ...   \n",
      "75%      518.67    642.900000   1593.440000   1412.680000  1.462000e+01  ...   \n",
      "max      518.67    645.110000   1616.910000   1441.490000  1.462000e+01  ...   \n",
      "\n",
      "          sensor_11     sensor_12     sensor_13     sensor_14     sensor_15  \\\n",
      "count  45351.000000  45351.000000  45351.000000  45351.000000  45351.000000   \n",
      "mean      47.472781    522.305988   2388.082793   8143.998115      8.417088   \n",
      "std        0.292293      2.586206      0.127003     17.721758      0.056212   \n",
      "min       46.690000    517.770000   2386.930000   8099.680000      8.156300   \n",
      "25%       47.250000    521.040000   2388.020000   8133.990000      8.386200   \n",
      "50%       47.440000    521.680000   2388.080000   8140.890000      8.421300   \n",
      "75%       47.660000    522.400000   2388.140000   8148.810000      8.453500   \n",
      "max       48.530000    537.400000   2388.610000   8293.720000      8.584800   \n",
      "\n",
      "          sensor_17  sensor_18  sensor_19     sensor_20     sensor_21  \n",
      "count  45351.000000    45351.0    45351.0  45351.000000  45351.000000  \n",
      "mean     392.859562     2388.0      100.0     38.910178     23.346022  \n",
      "std        1.698605        0.0        0.0      0.236600      0.141834  \n",
      "min      388.000000     2388.0      100.0     38.140000     22.872600  \n",
      "25%      392.000000     2388.0      100.0     38.760000     23.254500  \n",
      "50%      393.000000     2388.0      100.0     38.900000     23.342400  \n",
      "75%      394.000000     2388.0      100.0     39.050000     23.430100  \n",
      "max      400.000000     2388.0      100.0     39.850000     23.950500  \n",
      "\n",
      "[8 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e7012",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering: Remaining Useful Life (RUL) Calculation\n",
    "\n",
    "Predicting RUL requires a target variable that represents the time remaining until engine failure. For the C-MAPSS dataset, this is not directly provided but can be derived. The `max_cycles` function (from `custom_functions.py`) calculates the maximum `cycle` for each `engine_id` and then computes the RUL as `max_cycle_for_engine - current_cycle`. This creates a linearly decreasing target variable for each engine, going from its maximum operational cycle down to 0 at failure.\n",
    "\n",
    "The `head()` output below for `engine_id`, `cycle`, and `RUL` demonstrates this calculation for the first engine. As the `cycle` increases, the `RUL` value correctly decreases, starting from 191 cycles remaining down to 0 (which will be seen at the tail of the data for each engine). This is a critical step in preparing the target variable for our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93bbdbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   engine_id  cycle    RUL\n",
      "0        1.0      1  191.0\n",
      "1        1.0      2  190.0\n",
      "2        1.0      3  189.0\n",
      "3        1.0      4  188.0\n",
      "4        1.0      5  187.0\n"
     ]
    }
   ],
   "source": [
    "data = max_cycles(data)\n",
    "\n",
    "# Inspecting the RUL column\n",
    "print(data[['engine_id', 'cycle', 'RUL']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91754fc2",
   "metadata": {},
   "source": [
    "### 2.1 Verifying RUL at End-of-Life\n",
    "\n",
    "By inspecting the `tail()` of the combined dataset, we can observe the behavior of the `RUL` column, particularly for engines nearing their failure point. The output above shows the last few cycles of `engine_id` 609. As expected, the `RUL` value decreases to `0` at the final recorded cycle (`cycle` 255 for `engine_id` 609), confirming the correct calculation of the Remaining Useful Life. This validates that our target variable is appropriately defined for the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b826c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       engine_id  cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "45346        NaN    148       -0.0016       -0.0003         100.0    518.67   \n",
      "45347        NaN    149        0.0034       -0.0003         100.0    518.67   \n",
      "45348        NaN    150       -0.0016        0.0004         100.0    518.67   \n",
      "45349        NaN    151       -0.0023        0.0004         100.0    518.67   \n",
      "45350        NaN    152        0.0000        0.0003         100.0    518.67   \n",
      "\n",
      "       sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_12  sensor_13  \\\n",
      "45346    643.78   1596.01   1424.11     14.62  ...     519.66    2388.30   \n",
      "45347    643.29   1596.38   1429.14     14.62  ...     519.91    2388.28   \n",
      "45348    643.84   1604.53   1431.41     14.62  ...     519.44    2388.24   \n",
      "45349    643.94   1597.56   1426.57     14.62  ...     520.01    2388.26   \n",
      "45350    643.64   1599.04   1436.06     14.62  ...     519.48    2388.24   \n",
      "\n",
      "       sensor_14  sensor_15  sensor_17  sensor_18  sensor_19  sensor_20  \\\n",
      "45346    8138.08     8.5036        394       2388      100.0      38.44   \n",
      "45347    8144.36     8.5174        395       2388      100.0      38.50   \n",
      "45348    8135.95     8.5223        396       2388      100.0      38.39   \n",
      "45349    8141.24     8.5148        395       2388      100.0      38.31   \n",
      "45350    8136.98     8.5150        396       2388      100.0      38.56   \n",
      "\n",
      "       sensor_21  RUL  \n",
      "45346    22.9631  NaN  \n",
      "45347    22.9746  NaN  \n",
      "45348    23.0682  NaN  \n",
      "45349    23.0753  NaN  \n",
      "45350    23.0847  NaN  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Show df.tail() for an engine to verify RUL decreases to 0\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac0736",
   "metadata": {},
   "source": [
    "### 2.2 Final Columns After RUL Addition\n",
    "\n",
    "This output shows all the columns currently present in our consolidated training DataFrame, including the newly added `RUL` column. This confirms that all relevant features and the target variable are ready for the next stages of preprocessing, such as feature scaling and sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9366a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['engine_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3',\n",
      "       'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6',\n",
      "       'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12',\n",
      "       'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_18',\n",
      "       'sensor_19', 'sensor_20', 'sensor_21', 'RUL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the columns of dataset\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6d0f1",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing: Scaling and Sequence Generation\n",
    "\n",
    "Neural networks, especially LSTMs, perform best when input features are scaled to a common range. This prevents features with larger numerical values from dominating the learning process.\n",
    "\n",
    "### 3.1 Feature Scaling\n",
    "\n",
    "We use `MinMaxScaler` from `sklearn.preprocessing` to scale all operational settings and sensor measurements (`feature_cols`) to a range between 0 and 1. The `RUL` target variable is also scaled using a separate `MinMaxScaler` (`rul_scaler`). This is important because RUL values can be large, and scaling them helps the model converge faster and more stably.\n",
    "\n",
    "* **`feature_scaler.pkl`**: The scaler fitted on `feature_cols` from the training data is saved to ensure that the same scaling transformation can be applied to new, unseen test data.\n",
    "* **`rul_scaler.pkl`**: Similarly, the scaler for the `RUL` target variable is saved. This will be crucial for inverse-transforming the model's predicted RUL values back to their original scale for meaningful interpretation.\n",
    "\n",
    "The `head()` output above shows the training DataFrame after all features and the RUL target have been scaled. Notice how all values are now within the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d03f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']\n",
      "The Training Dataframe after Scaling:\n",
      "   engine_id     cycle  op_setting_1  op_setting_2  op_setting_3  sensor_1  \\\n",
      "0        1.0  0.000000      0.459770      0.153846           0.0       0.0   \n",
      "1        1.0  0.001908      0.609195      0.230769           0.0       0.0   \n",
      "2        1.0  0.003817      0.252874      0.692308           0.0       0.0   \n",
      "3        1.0  0.005725      0.540230      0.461538           0.0       0.0   \n",
      "4        1.0  0.007634      0.390805      0.307692           0.0       0.0   \n",
      "\n",
      "   sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_12  sensor_13  \\\n",
      "0  0.229508  0.482798  0.365358       0.0  ...   0.198166   0.648810   \n",
      "1  0.306792  0.523094  0.404780       0.0  ...   0.229750   0.678571   \n",
      "2  0.353630  0.450295  0.421232       0.0  ...   0.236882   0.654762   \n",
      "3  0.353630  0.351454  0.385069       0.0  ...   0.259297   0.684524   \n",
      "4  0.358314  0.352595  0.452584       0.0  ...   0.225166   0.660714   \n",
      "\n",
      "   sensor_14  sensor_15  sensor_17  sensor_18  sensor_19  sensor_20  \\\n",
      "0   0.200680   0.614236   0.333333        0.0        0.0   0.538012   \n",
      "1   0.163935   0.642940   0.333333        0.0        0.0   0.502924   \n",
      "2   0.172902   0.610268   0.166667        0.0        0.0   0.473684   \n",
      "3   0.175995   0.494516   0.333333        0.0        0.0   0.432749   \n",
      "4   0.175840   0.637340   0.416667        0.0        0.0   0.444444   \n",
      "\n",
      "   sensor_21       RUL  \n",
      "0   0.506912  0.364504  \n",
      "1   0.511179  0.362595  \n",
      "2   0.437517  0.360687  \n",
      "3   0.465071  0.358779  \n",
      "4   0.493367  0.356870  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Defining feature columns\n",
    "feature_cols = [col for col in data.columns if col not in ['RUL',\"engine_id\"]]\n",
    "print(feature_cols)\n",
    "\n",
    "# Scaling the features that will be used to train and test\n",
    "scaler = MinMaxScaler()\n",
    "data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
    "\n",
    "# Scaling the labels that will be used to train\n",
    "rul_scaler = MinMaxScaler()\n",
    "data['RUL'] = rul_scaler.fit_transform(data['RUL'].values.reshape(-1, 1))\n",
    "\n",
    "# Checking the dataframe after scaling\n",
    "print('The Training Dataframe after Scaling:')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162cdb5",
   "metadata": {},
   "source": [
    "### 3.2 Saving Scalers\n",
    "\n",
    "It is crucial to save the `MinMaxScaler` objects (`feature_scaler.pkl` and `rul_scaler.pkl`) after fitting them to the training data. This ensures that:\n",
    "1.  **Consistency:** The exact same scaling transformation (based on the training data's min/max values) can be applied to the test data or any future unseen data.\n",
    "2.  **Inverse Transformation:** The `rul_scaler` can be used to convert the model's predicted, scaled RUL values back into their original, interpretable cycle counts.\n",
    "\n",
    "This practice prevents data leakage from the test set and allows for consistent deployment of the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "414b40cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rul_scaler.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's good practice to save your scalers for later use on test data and for inverse transformation.\n",
    "# Example: saving the scalers (though not runnable without actual saving mechanism)\n",
    "import joblib\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(rul_scaler, 'rul_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c09a5",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Creating Rolling Window Statistics\n",
    "\n",
    "To capture the temporal dependencies and trends in the sensor data, we generate **rolling mean** and **rolling standard deviation** features for the selected sensor measurements. A `window_size` of 30 cycles is used. This means for each cycle, the rolling features are calculated based on the preceding 30 cycles (including the current one).\n",
    "\n",
    "* **Rolling Mean:** Provides a smoothed trend of the sensor readings, indicating general deterioration.\n",
    "* **Rolling Standard Deviation:** Captures the variability or volatility of sensor readings within the window, which can be an indicator of increasing instability as an engine degrades.\n",
    "\n",
    "The output above demonstrates the `sensor_2` original values alongside its 30-cycle rolling mean and standard deviation for the first engine. Notice how the rolling mean starts from the current value and gradually smooths out as more data points fill the window. The rolling standard deviation provides insight into the local variability of the sensor. These features are vital for LSTMs to learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87c7b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Displaying Rolling Features for Engine 1 (first 30 cycles) ---\n",
      "    engine_id     cycle  sensor_2  sensor_2_rolling_mean_30  \\\n",
      "0         1.0  0.000000  0.229508                  0.229508   \n",
      "1         1.0  0.001908  0.306792                  0.268150   \n",
      "2         1.0  0.003817  0.353630                  0.296643   \n",
      "3         1.0  0.005725  0.353630                  0.310890   \n",
      "4         1.0  0.007634  0.358314                  0.320375   \n",
      "5         1.0  0.009542  0.295082                  0.316159   \n",
      "6         1.0  0.011450  0.384075                  0.325861   \n",
      "7         1.0  0.013359  0.402810                  0.335480   \n",
      "8         1.0  0.015267  0.299766                  0.331512   \n",
      "9         1.0  0.017176  0.203747                  0.318735   \n",
      "10        1.0  0.019084  0.337237                  0.320417   \n",
      "11        1.0  0.020992  0.285714                  0.317525   \n",
      "12        1.0  0.022901  0.522248                  0.333273   \n",
      "13        1.0  0.024809  0.353630                  0.334727   \n",
      "14        1.0  0.026718  0.372365                  0.337237   \n",
      "\n",
      "    sensor_2_rolling_std_30  \n",
      "0                  0.054648  \n",
      "1                  0.054648  \n",
      "2                  0.062680  \n",
      "3                  0.058575  \n",
      "4                  0.054983  \n",
      "5                  0.050250  \n",
      "6                  0.052566  \n",
      "7                  0.055755  \n",
      "8                  0.053495  \n",
      "9                  0.064623  \n",
      "10                 0.061560  \n",
      "11                 0.059544  \n",
      "12                 0.080461  \n",
      "13                 0.077496  \n",
      "14                 0.075306  \n"
     ]
    }
   ],
   "source": [
    "# Defining the necessary variables for Forging Insights\n",
    "window_size = 30\n",
    "selected_sensors = [col for col in feature_cols if col not in ['cycle','op_setting_1','op_setting_2','op_setting_3']]\n",
    "\n",
    "data = rolling_mean_std(data,window_size,feature_cols)\n",
    "\n",
    "print(\"\\n--- Displaying Rolling Features for Engine 1 (first 30 cycles) ---\")\n",
    "# Show how the rolling features look for engine 1\n",
    "engine1_df = data[data['engine_id'] == 1]\n",
    "print(engine1_df[['engine_id', 'cycle', 'sensor_2', f'sensor_2_rolling_mean_{window_size}', f'sensor_2_rolling_std_{window_size}']].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab9768",
   "metadata": {},
   "source": [
    "### 4.1 Updated Feature Set for Model Training\n",
    "\n",
    "After generating the rolling mean and standard deviation for all relevant sensor features, our feature set (`feature_cols`) has significantly expanded. The output below confirms the new list of features that will be used as input to the LSTM model. It now includes the original `cycle` and `op_setting` features, plus the original sensor readings, and their corresponding rolling mean and standard deviation features. This comprehensive set aims to provide the model with a rich representation of the engine's health over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1d5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21', 'sensor_1_rolling_mean_30', 'sensor_1_rolling_std_30', 'sensor_2_rolling_mean_30', 'sensor_2_rolling_std_30', 'sensor_3_rolling_mean_30', 'sensor_3_rolling_std_30', 'sensor_4_rolling_mean_30', 'sensor_4_rolling_std_30', 'sensor_5_rolling_mean_30', 'sensor_5_rolling_std_30', 'sensor_6_rolling_mean_30', 'sensor_6_rolling_std_30', 'sensor_7_rolling_mean_30', 'sensor_7_rolling_std_30', 'sensor_8_rolling_mean_30', 'sensor_8_rolling_std_30', 'sensor_9_rolling_mean_30', 'sensor_9_rolling_std_30', 'sensor_11_rolling_mean_30', 'sensor_11_rolling_std_30', 'sensor_12_rolling_mean_30', 'sensor_12_rolling_std_30', 'sensor_13_rolling_mean_30', 'sensor_13_rolling_std_30', 'sensor_14_rolling_mean_30', 'sensor_14_rolling_std_30', 'sensor_15_rolling_mean_30', 'sensor_15_rolling_std_30', 'sensor_17_rolling_mean_30', 'sensor_17_rolling_std_30', 'sensor_18_rolling_mean_30', 'sensor_18_rolling_std_30', 'sensor_19_rolling_mean_30', 'sensor_19_rolling_std_30', 'sensor_20_rolling_mean_30', 'sensor_20_rolling_std_30', 'sensor_21_rolling_mean_30', 'sensor_21_rolling_std_30']\n",
      "(61)\n"
     ]
    }
   ],
   "source": [
    "# updating the feature_cols\n",
    "feature_cols = [col for col in data.columns if col not in ['engine_id','RUL']]\n",
    "print(feature_cols)\n",
    "print(f'({len(feature_cols)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70c2bd",
   "metadata": {},
   "source": [
    "## 5. Sequence Generation for LSTM Model\n",
    "\n",
    "LSTMs require input data to be in a sequence format (samples, timesteps, features). For our RUL prediction task, each sequence represents a fixed \"look-back window\" of an engine's operational history. The `create_sequences` function (from `custom_functions.py`) is used to transform our flattened DataFrame into this 3D format.\n",
    "\n",
    "For each engine:\n",
    "1.  It iterates through the engine's data, creating sequences of `sequence_length` (e.g., 30) cycles.\n",
    "2.  Each sequence's target label is the RUL value at the *end* of that sequence.\n",
    "\n",
    "This approach allows the LSTM to learn the temporal patterns leading up to a specific RUL value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "711a1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence length (hyperparameter)\n",
    "sequence_length = 30 # Let's start with a look-back window of cycles\n",
    "\n",
    "# Initialize empty lists to store all sequences and labels from all engines\n",
    "X_train_sequences = []\n",
    "y_train_labels = []\n",
    "\n",
    "# Group the DataFrame by engine_id and iterate through each group (each engine)\n",
    "# This is crucial to keep sequences from different engines separate\n",
    "for engine_id, engine_df in data.groupby('engine_id'):\n",
    "    # Generate sequences and labels for the current engine\n",
    "    sequences_X, labels_y = create_sequences(engine_df, sequence_length, feature_cols)\n",
    "    \n",
    "    # Extend the main lists with the sequences and labels from this engine\n",
    "    X_train_sequences.extend(sequences_X)\n",
    "    y_train_labels.extend(labels_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c4cf5",
   "metadata": {},
   "source": [
    "### 5.2 Final Data Shapes for LSTM Input\n",
    "\n",
    "After sequence generation, the data is converted into NumPy arrays, which is the required input format for Keras/TensorFlow models.\n",
    "\n",
    "The output confirms the shapes of our processed arrays:\n",
    "* **`X_train` (Training Sequences):** `(142698, 30, 61)`\n",
    "    * **142698:** Number of training samples (sequences).\n",
    "    * **30:** `sequence_length` (timesteps per sequence). This means each input to the LSTM will consist of 30 historical cycles.\n",
    "    * **61:** Number of features per timestep (the `feature_cols`).\n",
    "* **`y_train` (Training Labels):** `(142698, 1)`\n",
    "    * **142698:** Number of corresponding RUL labels for each sequence.\n",
    "    * **1:** Each label is a single RUL value.\n",
    "* **`X_test` (Test Sequences):** Will have a shape like `(num_test_engines, 30, 61)` (though not explicitly printed here, it's inferred).\n",
    "\n",
    "These reshaped arrays are now in the correct format to be fed into an LSTM neural network for training and prediction. The saving of `X_train.npy`, `X_test.npy`, and `y_train.npy` ensures that these preprocessed datasets can be easily loaded for model training without re-running the entire preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e734e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (sequences): (35462, 30, 61)\n",
      "Shape of y_train (RUL labels): (35462, 1)\n",
      "\n",
      "First sequence from X_train:\n",
      "[[0.         0.45977011 0.15384615 ... 0.02481076 0.50691159 0.00301762]\n",
      " [0.0019084  0.6091954  0.23076923 ... 0.02481076 0.50904537 0.00301762]\n",
      " [0.00381679 0.25287356 0.69230769 ... 0.03220801 0.48520271 0.04135178]\n",
      " ...\n",
      " [0.05152672 0.36206897 0.84615385 ... 0.05019797 0.47580282 0.05024965]\n",
      " [0.05343511 0.56896552 0.38461538 ... 0.04949831 0.47497849 0.04954346]\n",
      " [0.05534351 0.37356322 0.46153846 ... 0.04938171 0.47579553 0.04888702]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the lists into NumPy arrays for Keras\n",
    "# This is where we get our 3D array (samples, timesteps, features)\n",
    "X_train = np.array(X_train_sequences)\n",
    "y_train = np.array(y_train_labels)\n",
    "\n",
    "# Saving The Processed Test set for late use\n",
    "np.save('X_train.npy', X_train)\n",
    "\n",
    "# Reshape y_train to be 2D for model fitting (if not already)\n",
    "# This step is good practice to ensure compatibility with Keras\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "# Checking all the Outputs of create_sequences\n",
    "print(\"Shape of X_train (sequences):\", X_train.shape)\n",
    "print(\"Shape of y_train (RUL labels):\", y_train.shape)\n",
    "print(\"\\nFirst sequence from X_train:\")\n",
    "print(X_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
