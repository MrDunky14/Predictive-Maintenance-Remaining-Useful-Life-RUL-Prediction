{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5889898",
   "metadata": {},
   "source": [
    "# Predictive Maintenance: Turbofan Engine Remaining Useful Life (RUL) Prediction - Model Retraining/Fine-tuning\n",
    "\n",
    "This notebook focuses on the **Re-Training** of an already deployed or pre-trained LSTM model for Remaining Useful Life (RUL) prediction. In real-world scenarios, models often need to be updated with new data or adapted to slightly different conditions without starting training from scratch. Fine-tuning allows us to leverage the knowledge gained by the pre-trained model and efficiently update it.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Setup and Data Loading](#1.-Setup-and-Data-Loading)\n",
    "    * [1.1 Importing Libraries and Loading Preprocessed Data](#1.1-Importing-Libraries-and-Loading-Preprocessed-Data)\n",
    "    * [1.2 Loading the Pre-trained Model and Scalers](#1.2-Loading-the-Pre-trained-Model-and-Scalers)\n",
    "2.  [Model Re-Training](#2.-Model-Re-Training)\n",
    "    * [2.1 Recompiling for Re-Training](#2.1-Recompiling-for-Re-Training)\n",
    "    * [2.2 Re-Training the Model](#2.2-Re-Training-the-Model)\n",
    "3.  [Saving the Re-Trained Model](#3.-Saving-the-Re-Trained-Model)\n",
    "4.  [Conclusion](#4.-Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from custom_functions import *\n",
    "from custom_functions import PHM2008Score\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d0931",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "This section handles the necessary imports and loads the preprocessed data, along with the pre-trained model and scalers, preparing the environment for the fine-tuning process.\n",
    "\n",
    "### 1.1 Importing Libraries and Loading Preprocessed Data\n",
    "\n",
    "We begin by importing all essential libraries for model operations, including `tensorflow` for the deep learning model, `numpy` for numerical operations, `pandas` for data handling, and `joblib` for loading pre-saved scalers. Crucially, we also import `PHM2008Score` from `custom_functions`, which is a custom metric relevant to this predictive maintenance challenge.\n",
    "\n",
    "The preprocessed `X_train` and `y_train` NumPy arrays, generated from the `EDA.ipynb` notebook, are loaded. These arrays represent the time-series sequences and their corresponding RUL values, ready for model training. The `rul_scaler` and `feature_scaler` are also loaded to ensure consistent data transformation.\n",
    "\n",
    "### 1.2 Loading the Pre-trained Model and Scalers\n",
    "\n",
    "Before fine-tuning, we load the pre-trained LSTM model, which was previously optimized (e.g., from `Model_Tuner.ipynb`). This model has already learned general degradation patterns from the initial training data. Fine-tuning will allow us to adapt this model to potentially new data or refine its performance without rebuilding from scratch.\n",
    "\n",
    "The output confirms that the pre-trained model has been successfully loaded. This model will now serve as the starting point for the retraining process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "rul_scaler = joblib.load('rul_scaler.pkl')\n",
    "scaler = joblib.load('feature_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa696462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Your Deployed Model and Scalers ---\n",
    "model_save_path_keras = \"final_lstm_model.keras\" # Path to your deployed model\n",
    "fine_tune_model = tf.keras.models.load_model(model_save_path_keras)\n",
    "print(\"Pre-trained model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fine-tune the Model ---\n",
    "print(\"\\n--- Starting Fine-tuning Training ---\")\n",
    "\n",
    "# Recompile the model with a much smaller learning rate\n",
    "# This prevents drastic changes to already learned weights\n",
    "new_learning_rate = 0.0000436 # A common practice: use 1/10th or 1/100th of your original LR (Here its 1/10)\n",
    "fine_tune_model.compile(optimizer=Adam(learning_rate=new_learning_rate),\n",
    "                        loss='mae',\n",
    "                        metrics=['mae', PHM2008Score()])\n",
    "\n",
    "# Callbacks for fine-tuning\n",
    "\n",
    "# Create a callback to stop training when a metric has stopped improving\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_phm_2008_score', patience=10, mode='min', restore_best_weights=True)\n",
    "\n",
    "fine_tuned_model_filepath = 'fine_tuned_lstm_model_v2.keras' # New filename for fine-tuned model\n",
    "model_checkpoint_ft = ModelCheckpoint(\n",
    "    filepath=fine_tuned_model_filepath,\n",
    "    monitor='val_phm_2008_score',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "callbacks_list_ft = [stop_early, model_checkpoint_ft]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ce65d",
   "metadata": {},
   "source": [
    "## 2. Model Re-Training\n",
    "\n",
    "Fine-tuning involves continuing the training of an already trained model on new or existing data, typically with a much smaller learning rate. This allows the model to adjust its weights subtly without forgetting the extensive knowledge it gained during initial training.\n",
    "\n",
    "### 2.1 Recompiling for Re-Training\n",
    "\n",
    "A critical step in fine-tuning is to **recompile the model with a significantly smaller learning rate**. This prevents large updates to the weights that could destabilize the already learned features. A common practice is to use a learning rate that is 1/10th or 1/100th of the original learning rate. Here, a new learning rate of `0.0000436` is set.\n",
    "\n",
    "We also recompile with `loss='mae'` (Mean Absolute Error) and include `PHM2008Score()` as a metric. This ensures that the fine-tuning process directly optimizes for the MAE and monitors the challenge-specific PHM08 score, which is crucial for evaluating RUL predictions.\n",
    "\n",
    "Additionally, `EarlyStopping` and `ModelCheckpoint` callbacks are configured specifically for the fine-tuning phase:\n",
    "\n",
    "* **`EarlyStopping`**: Monitors `val_phm_2008_score` (validation PHM 2008 score) and stops training if it doesn't improve for 10 consecutive epochs, restoring the best weights. This ensures we stop at the optimal point for the specific challenge metric.\n",
    "\n",
    "* **`ModelCheckpoint`**: Saves the best model based on the `val_phm_2008_score` to a new file, `fine_tuned_lstm_model_v2.keras`, preserving the best fine-tuned version.\n",
    "\n",
    "### 2.2 Re-Training the Model\n",
    "\n",
    "The `fine_tune_model` is now trained on the `X_train` and `y_train` data. Although the model is already trained, this step allows it to further optimize its weights based on the specified fine-tuning parameters and callbacks.\n",
    "\n",
    "* **`epochs=100`**: A maximum of 100 epochs is set, but `EarlyStopping` will likely halt training sooner if the `val_phm_2008_score` does not improve.\n",
    "\n",
    "* **`validation_data=0.2`**: A validation split of 20% is used to monitor performance on unseen data during fine-tuning, guiding the `EarlyStopping` and `ModelCheckpoint` callbacks.\n",
    "\n",
    "* **`batch_size=2048`**: A larger batch size is used, which can sometimes speed up training for fine-tuning, though it might require careful tuning.\n",
    "\n",
    "The output will show the training progress, including loss and metrics for both training and validation sets, indicating how the model adapts during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the best model with optimal hyperparameters ---\n",
    "print(\"\\nTraining the best model with re_train_data...\")\n",
    "\n",
    "# Train the final model (you might use more epochs here)\n",
    "history = fine_tune_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=100, # Train for more epochs\n",
    "    validation_data=0.2,\n",
    "    callbacks=[callbacks_list_ft], # Use early stopping again\n",
    "    batch_size=2048,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nFinal model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd2930",
   "metadata": {},
   "source": [
    "## 3. Saving the Re-Trained Model\n",
    "\n",
    "After the Re-training process is complete, the final `Re-trained model` (which has its weights restored to the best performing epoch during fine-tuning) is saved.\n",
    "\n",
    "This saved model (`model/final_model.keras`) represents the updated, optimized version of your predictive maintenance solution. It can now be used for inference on new, real-world data, benefiting from both the initial extensive training and the targeted fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in .keras format\n",
    "model_save_path_keras = \"model/final_model.keras\"\n",
    "fine_tune_model.save(model_save_path_keras)\n",
    "print(f\"Final model saved in .keras format to: {model_save_path_keras}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22ebfc",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "This notebook successfully demonstrates a practical approach to **model retraining and fine-tuning** for predictive maintenance. By loading a pre-trained model and continuing its training with a carefully chosen learning rate and relevant callbacks, we can efficiently adapt and improve the model's performance. This process is crucial for maintaining model relevance and accuracy in dynamic real-world environments where new data becomes available or conditions change.\n",
    "\n",
    "Fine-tuning allows for:\n",
    "* **Leveraging Pre-trained Knowledge:** Avoiding training from scratch, saving computational resources and time.\n",
    "* **Adapting to New Data:** Incorporating new degradation patterns or operational conditions.\n",
    "* **Refining Performance:** Achieving incremental improvements on specific metrics.\n",
    "\n",
    "This showcases an important aspect of a robust machine learning lifecycle, demonstrating your ability to deploy and maintain models effectively."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
